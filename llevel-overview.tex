%!TEX root = main.tex

\section{$L$-level Recommendation Policy}
\label{sec:llevel}
In this section, we give an overview of how we extend our 3-level policy to an $L$-level policy for $L > 3$ to achieve better regret. Detailed proofs and discussions can be found in Appendix \ref{sec:llevel-details}.

%\jmcomment{Will change these statements a bit.}
\begin{theorem}
\label{thm:llevel-1}
For any $L > 3$, we have an $L$-level recommendation policy with expected regret \\$O\left(T^{2^{L-1}/(2^L-1)} polylog(T) \right)$. In particular, we have an $O(\log\log(T))$-level recommendation policy with expected regret $O(T^{1/2} polylog(T))$. 
\end{theorem}


\begin{theorem}
\label{thm:llevel-2}
 We have an $O(\log(T)/\log\log(T))$-level recommendation policy with expected regret $O(\min(1/\Delta, T^{1/2})polylog(T))$. Here $\Delta$ is the difference between the largest mean and second largest mean of arms and the recommendation policy does not depend on $\Delta$. Moreover, agent in round $t$ observes a subhistory of size at least $\Omega( \lfloor t/polylog(T)\rfloor)$. 
\end{theorem}

Now we give some overview of the additional techniques we use to get our $L$-level results. 

\xhdr{New connecting structures between levels.}

\xhdr{Additional groups for boundary cases.}