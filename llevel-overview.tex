%!TEX root = main.tex

\section{$L$-level Recommendation Policy}
\label{sec:llevel}
In this section, we give an overview of how we extend our 3-level policy to an $L$-level policy for $L > 3$ in order to achieve better regret. In Theorem \ref{thm:llevel-1}, we show our $L$-level policy. We can achieve nearly optimal regret $O(T^{1/2} polylog(T))$ with $O(\log\log(T))$ levels.
\begin{theorem}
\label{thm:llevel-1}
For any $L > 3$, we have an $L$-level recommendation policy with regret \\$O\left(T^{2^{L-1}/(2^L-1)} polylog(T) \right)$. In particular, we have an $O(\log\log(T))$-level recommendation policy with regret $O(T^{1/2} polylog(T))$. 
\end{theorem}

In Theorem \ref{thm:llevel-2}, we show a recommendation policy with instance-dependent regret guarantee. This policy has the same structure as the one in Theorem \ref{thm:llevel-1} but different parameters. Its regret depends on $\Delta$ which is the difference between the largest mean and second largest mean of arms and its construction does not depend on $\Delta$. Its regret bound outperforms the one in Theorem \ref{thm:llevel-1} when $\Delta$ is much bigger than $T^{-1/2}$. The only downside is that this policy requires more levels, i.e. $O(\log(T)/\log\log(T))$ levels. It also has the property that each agent observes a good fraction of history till its round. 

\begin{theorem}
\label{thm:llevel-2}
 We have an $O(\log(T)/\log\log(T))$-level recommendation policy with regret \\$O(\min(1/\Delta, T^{1/2})polylog(T))$. Here $\Delta$ is the difference between the largest mean and second largest mean of arms. The recommendation policy does not depend on $\Delta$. Moreover, agent in round $t$ observes a subhistory of size at least $\Omega( \lfloor t/polylog(T)\rfloor)$. 
\end{theorem}

Detailed proofs and discussions of Theorem \ref{thm:llevel-1} and Theorem \ref{thm:llevel-2} can be found in Appendix \ref{sec:llevel-details}. Similarly as Section \ref{sec:3level}, we first prove them in the case of 2 arms (Theorem \ref{thm:llevel} and Corollary \ref{cor:llevel}). We then extend them to the case of constant number of arms (Theorem \ref{thm:constarm}).

The main idea of extending from 3-level to $L$-level is that instead of using the second level as one ``check-point'', we use more levels to have multiple ``check points''.  Some new challenges appear in this process. Here we give an overview of the additional techniques we use to get our $L$-level results. 

\xhdr{Connecting structures between levels.} In our 3-level policy, the second level has $S = \Theta(\log(T))$ groups. We do so to ensure that in the small gap case (Lemma \ref{3levelsmallcase}), with high probability, both arms are pulled enough times in the second level. The argument relies on the fact that agents in different second-level groups observe disjoint histories of the first level and the independent randomness in first level groups guarantees each arm $a$ to have a ``lucky'' second-level group such that agents in that group all pull arm $a$ with high probability. 

Simply generalizing this idea to an $L$-level policy would give us a $S$-ary tree like structure and the first level will have $S^{L-1}$ groups. It incurs an extra $S^{\Omega(L)}$ factor in the regret. If we use $\log\log(T)$ levels as in Theorem \ref{thm:llevel-1}, this factor super poly-logarithmic. If we use $\log(T)/\log\log(T)$ levels as in Theorem \ref{thm:llevel-2}, this factor goes up to polynomial in $T$. 

In order to avoid this undesirable factor, we design a slightly different connecting structure between levels in our $L$-level policy. The key observation is that, for any level $l \in \{3,...,L-1\}$, we do want agents in different $l$-th level groups to see disjoint histories of the $(l-1)$-th level to ensure that there is some ``lucky'' group for each arm whose mean is close enough to the best arm. However, it does not matter much if pulls in levels below $l-1$ get observed by agents in different $l$-th level groups because group sizes in lower levels are much smaller than group sizes of level $l-1$ and the independent randomness in level $l-1$ is sufficient.

For each level, our $L$-level policy has $S^2$ groups for some $S = \Theta(\log(T))$. The groups in the $l$-th level are labeled as $G_{l,u,v}$ for $u,v\in[S]$. For any $l \in \{2,...,L\}$ and $u,v,w\in [S]$, agents in group $G_{l,u,v}$ see the history of agents in group $G_{l-1,v,w}$ (and by transitivity all agents in levels below $l-1$). See Figure \ref{fig:llevel-connecting} for a graphical view (as space limit, we only show 3 levels and 4 groups in each level). This connecting structure has three nice properties: \\
(i) The number of groups does not grow with $L$ and stays as $S^2 = \Theta(\log^2(T))$ in each level. Having these groups only incurs a $\Theta(\log^2(T))$ multiplicative factor to the regret.\\
(ii) Consider groups $G_{l-1,1,v},G_{l-1,2,v},...,G_{l-1,S,v}$ (e.g. $G_{l-1,1,1}$ and $G_{l-1,2,1}$ with red circles in the figure ) as a set of groups indexed by $l-1$ and $v$. The agents in a set of groups see the same history. They get ``lucky'' for some arm at the same time.\\
(iii) For each set of groups in level $l-1$, agents in level $l$ see the history of agents in one of these groups. Therefore, for each arm $a$, we just need one set of groups in level $l-1$ to get ``lucky'' and then all agents in level $l$ will see enough pulls of arm $a$. 

\begin{figure}[H]
\centering
\begin{tikzpicture}  
 \foreach \x in {0,3,6,9}
 {
 \filldraw[fill=purple!20!white]
 (\x+0,7.5)--(\x+2,7.5)--(\x+2,8.5)--(\x+0,8.5)--cycle;
 \filldraw[fill=green!20!white]
 (\x+0,5)--(\x+2,5)--(\x+2,6)--(\x+0,6)--cycle;
 \filldraw[fill=blue!20!white]
 (\x+0,2.5)--(\x+2,2.5)--(\x+2,3.5)--(\x+0,3.5)--cycle;
 }
\foreach \y in {2.5,5,7.5}
{
  \node at (12.5,\y+0.5){$\cdots$};
} 


\foreach \y in {3.5,6}
{
  \draw (1,\y) -- (1,\y+1.5);
  \draw (1,\y) -- (7,\y+1.5);
  
  \draw (4,\y) -- (1,\y+1.5);
  \draw (4,\y) -- (7,\y+1.5);
  
  
  \draw (7,\y) -- (4,\y+1.5);
  \draw (7,\y) -- (10,\y+1.5);
  
  
  \draw (10,\y) -- (4,\y+1.5);
  \draw (10,\y) -- (10,\y+1.5);
} 

\foreach \u in {1,2}
{
	\foreach \v in {1,2}
	{
	\pgfmathsetmacro{\x}{((\u-1)*2+(\v-1))*3};
	\pgfmathsetmacro{\xa}{((\u-1))*3};
	\pgfmathsetmacro{\xb}{(2+(\u-1))*3};
   \node at(\x+1,8){$G_{l,\u,\v}$};
   \node at(\x+1,5.5){$G_{l-1,\u,\v}$};
   \node at(\x+1,3){$G_{l-2,\u,\v}$};
	}
}


  \node at(-1.2,3){\textbf{Level $l-2$}};
  \node at(-1.2,5.5){\textbf{Level $l-1$}};
  \node at(-1.2,8){\textbf{Level $l$}};
  \draw[->] (13.3,2)--(13.3,8.5);
  \node at(13.7,5)[ rotate=90]{Time};

 \draw [rounded corners=3mm, line width=1mm, red](-0.2,4.8)--(2.2,4.8)--(2.2,6.2)--(-0.2,6.2)--cycle;
  \draw [rounded corners=3mm, line width=1mm, red](5.8,4.8)--(8.2,4.8)--(8.2,6.2)--(5.8,6.2)--cycle;
\end{tikzpicture}
\caption{Connectiing structures between levels for the $L$-level policy.}
\label{fig:llevel-connecting}
\end{figure}
\xhdr{Additional groups for boundary cases.} Recall in our 3-level policy, the medium gap case (Lemma \ref{3levelmedium}) is the boundary case when the gap $|\mu_1-\mu_2|$ is $\Omega\left(\sqrt{\frac{1}{T_1}}\right)$ and $O\left(\sqrt{\frac{\log(T)}{T_1}}\right)$. In this case, the gap is not large enough to conclude that with high probability agents in both the second level and the third level all pull the best arm. The gap is also not small enough to conclude that with high probability both arms are explored enough times in the second level. We need to worry about the situation in which the sub-optimal arm does not get pulled enough times in the first two levels and some agents in the third level may pull the sub-optimal arm. Such situation is naturally ruled out in our 3-level policy for the following reason. Agents in the third level observe the history of the entire first level while agents in the second level only observes the history of a single first-level group. For each arm, even if it is not explored enough times in the second level, its empirical mean concentrates closer to its actual mean in the history observed by a third-level agent than the one observed by a second-level agent. Therefore, although the gap in the medium gap case is just not large enough for agents in the second level to all pull the best arm, it is large enough for agents in the third level to all pull the best arm.

When we extend the 3-level policy to an $L$-level policy, we have such boundary case for each intermediate level. Moreover, the worry mentioned above does not get naturally resolved as in some of these boundary cases, the ratios between the upper and lower bounds of the gap rise from $\Theta(\sqrt{\log(T)})$ to $\Theta(\log(T))$. The reason for such rise is that, except the first level, we don't have good enough guarantee of the number of pulls of each arm. For example, in Figure \ref{fig:llevel-connecting}, when we talk about having enough arm $a$ pulls in the history observed by agents in $G_{l,1,1}$, it could be that  only agents in group $G_{l-1,1,1}$ are pulling arm $a$ and it also could be that most agents in groups $G_{l-1,1,1},G_{l-1,1,2},...,G_{l-1,1,S}$ are pulling arm $a$. Therefore our estimate of the number of arm $a$ pulls can be off by an $S=\Theta(\log(T))$ multiplicative factor. This finally makes the boundary cases harder to deal with.

In our $L$-level policy, after understanding the boundary cases well, we resolve this problem by introducing an additional type of groups: $\Gamma$-groups. For each $l \in [L], u,v \in [S]$, we create a $\Gamma$-group $\Gamma_{l,u,v}$. Agents in $\Gamma_{l,u,v}$ observe the same history as the one observed by agents in $G_{l,u,v}$ and the number of agents in $\Gamma_{l,u,v}$ is $\Theta(\log(T)$ times the number of agents in $G_{l,u,v}$. The main difference between the $G$-groups and $\Gamma$-groups is that the history of $\Gamma$ groups in level $l$ is not sent to agents in level $l+1$ but agents in higher levels. When we are in the boundary case in which we don't have good guarantees about the $l+1$ level agents' pulls, the new construction makes sure that agents in levels higher than $l+1$ get enough pulls of each arm and all pull the best arm. 