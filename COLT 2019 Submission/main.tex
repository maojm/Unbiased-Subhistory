\documentclass[anon,12pt]{colt2019}
%\usepackage{fullpage}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage{times}
% ======    PACKAGES
\usepackage{slivkins-setup}
%,slivkins-theorems}
%\usepackage{amsmath, amsfonts, amssymb, amsthm, amsbsy, amscd, bm, bbm}
%\usepackage{amsbsy,amscd, bm, bbm}
\usepackage{kpfonts}
\usepackage{array}
%\usepackage{booktabs}
\usepackage{graphicx}
%\usepackage[small,bf]{caption}
%\setlength{\captionmargin}{30pt}
%\usepackage{subcaption}
%\captionsetup[sub]{margin=10pt,font=small}
\usepackage{color}
\usepackage{ifthen}
\usepackage{xspace}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
%\usepackage[colorlinks,citecolor={black},urlcolor={black},linkcolor={black}]{hyperref}
\usepackage{url}
%\usepackage{tocbibind}
\usepackage{enumerate}
\usepackage{mdframed}
\usepackage{comment}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{cleveref}

\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{claim}[theorem]{Claim}
\DeclareMathOperator*{\argmin}{argmin}

% a very useful package for edits and comments, from David Kempe (USC)
%\usepackage{color-edits}
%\usepackage[suppress]{color-edits}  % use this to suppress the package
%\addauthor{as}{red}      % as for Alex
%\addauthor{jm}{blue}     % jm for Jieming
%\addauthor{ni}{magenta}     % ni for Nicole
%\addauthor{sw}{brown}     % sw for Steven
% e.g. for Alex, provides \asedit{}, \ascomment{} and \asdelete{}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{conjecture}[theorem]{Conjecture}
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{remark}{Remark}[section]
% \newtheorem{claim}{Claim}[section]
% \newtheorem{example}{Example}[section]


\newcommand{\term}[1]{\ensuremath{\mathtt{#1}}\xspace}
\newcommand{\thread}{\term{thread}}
\newcommand{\ALGG}{full-disclosure path }
\newcommand{\ALGPG}{\term{Parallel Greedy}}

% Alex's notation!
\newcommand{\SubH}[1]{\mathcal{H}_{#1}} % subhistory
\newcommand{\AnonSubH}[1]{H^{\texttt{anon}}_{#1}} % subhistory

% new notation for FDPs (full-disclosure paths)
\newcommand{\fdp}{\term{FDP}}
\newcommand{\fdpL}{L^\fdp_K} % sufficient length of greedy path
\newcommand{\fdpP}{p^\fdp_K} % success prob for greedy path
\newcommand{\fdpN}[1][a]{N^\fdp_{K,#1}}
    % Exp #pulls for arm $a$ in greedy path of length L_K.

% new notation for low-deviation assumption on agents' reward estimates
\newcommand{\estN}{N_{\term{est}}}
\newcommand{\estC}{C_{\term{est}}}

% notation for 3level section
\newcommand{\conf}[1]{\mathtt{conf}\left(#1\right)}
\newcommand{\event}[1]{\ensuremath{\mathtt{event}_{#1}}\xspace}
\newcommand{\cG}{\mathcal{G}}

% Greedy notation
\def\GdT{L^\fdp_K} % sufficient length of greedy path
\def\GdP{p^\fdp_K} % success prob for greedy path

\def\2LEVEL{two-level policy}

\newcommand{\DT}{\mD} % Alex' notation for distribution over types.


%number of groups
\def\NG{\sigma}

\def\DKL{\textbf{D}_{\term{KL}}}
\def\D{\mathbb{D}}
\def\E{\mathbb{E}}
\def\reg{\mathrm{Reg}}
\def\A{\mathcal{A}}
\def\M{\mathcal{M}}
\def\S{\mathcal{S}}
\def\X{\mathcal{X}}
\def\EX{\term{EX}}
\def\Ds{*}
\def\EE{\mathcal{E}}
\def\varTheta{\bold{\Theta}}
\def\varOmega{\bold{\Omega}}
\def\cD{\mathcal{D}}
\def\cT{\mathcal{T}}
\def\Ber{\emph{Ber} }

\def\pmn{\hat{\mu}}
\def\emn{\bar{\mu}}
%\def\q_a{\fdpN}
%\def\ell{l}

%index on the tape
\def\z{\tau}
\title{Incentivizing Exploration with Unbiased Histories}

% Authors with different addresses:
\coltauthor{
\Name{Nicole Immorlica} \Email{nicimm@microsoft.com}\\
\addr Microsoft Research, Cambridge, MA.
\and
\Name{Jieming Mao} \Email{maojm517@gmail.com}\\
\addr University of Pennsylvania, Philadelphia, PA.
\and
\Name{Aleksandrs Slivkins}  \Email{slivkins@microsoft.com}\\
\addr Microsoft Research, New York, NY.
\and
\Name{Zhiwei Steven Wu} \Email{zsw@umn.edu}\\
\addr University of Minnesota, Minneapolis, MN.
}
\begin{document}
\begin{titlepage}
\maketitle

\thispagestyle{empty}
\begin{abstract}
In a social learning setting, there is a set of actions, each of which has a payoff that depends on a hidden state of the world. A sequence of agents each chooses an action with the goal of maximizing payoff given estimates of the state of the world.  A disclosure policy tries to coordinate the choices of the agents by sending messages about the history of past actions.  The goal of the algorithm is to minimize the regret of the action sequence.

In this paper, we study a particular class of disclosure policies that use messages, called {\em unbiased subhistories}, consisting of the actions and rewards from by a subsequence of past agents, where the subsequence is chosen ahead of time. One trivial message of this form contains the full history; a disclosure policy that chooses to use such messages risks inducing herding behavior among the agents and thus has regret linear in the number of rounds.  Our main result is a disclosure policy using unbiased subhistories that obtains regret $\tilde{O}(\sqrt{T})$.  We also exhibit simpler policies with higher, but still sublinear, regret.  These policies can be interpreted as dividing a sublinear number of agents into constant-sized focus groups, whose histories are then fed to future agents.
\end{abstract}
\begin{keywords}%
  incentivizing exploration, multi-armed bandit, social learning
\end{keywords}
\end{titlepage}
\input{intro}
\input{model}

%\input{algg}

\input{2level}

\input{3level}

%l-level in the 10page
\input{llevel-overview}

%\bibliographystyle{plain}
%

%\acks{We would like to thank Robert Kleinberg for discussions in the early stage of this project.}
\bibliography{bib-abbrv,bib-AGT,bib-bandits,bib-ML,bib-random,bib-slivkins}
\appendix
\input{prelim}
\input{app-warmup}
\input{3levelproofs}
\input{llevel}


%\input{posterior}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
