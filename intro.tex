\section{Introduction}
\label{sec:intro}

Recommendation systems are ubiquitous, providing recommendations for movies (\eg  Netflix), products (\eg  Amazon), restaurants (\eg  Yelp), vacations (\eg Tripadvisor), etc.  A typical recommendation system elicits user feedback about their experiences, and aggregates this feedback in order to provide better recommendations in the future. Thus, each user she consumes information from the previous users (indirectly, \eg via recommendations), and produces new information (\eg  a review) that benefits future users. This dual role creates a three-way tension between exploration, exploitation, and users' incentives.
A social planner would balance ``exploration" of insufficiently known alternatives and ``exploitation" of the information acquired so far. Designing algorithms to trade off these two objectives is a well-researched subject in machine learning and operations research. However, a given user who decides to ``explore" typically suffers all the downside of this decision, whereas the upside (improved recommendations) is spread over many users in the future. Therefore, users' incentives are skewed in favor of exploitation. As a result, observations may be collected at a slower rate, and may suffer from selection bias (\eg  ratings of a particular movie may mostly come from people who like this type of movies). Moreover, in some natural but idealized models (\eg  \cite{Kremer-JPE14,ICexploration-ec15}), there are simple  examples when optimal recommendations are never found because the corresponding actions are never taken.

Thus, we have a problem of \emph{incentivizing exploration}. Providing monetary incentives can be financially or technologically unfeasible, and relying on voluntary exploration can lead to selection biases. A recent line of work, started by \cite{Kremer-JPE14}, relies on the inherent \emph{information asymmetry} between the recommendation system and a user. These papers posit a simple model, termed \emph{Bayesian Exploration} in \cite{ICexplorationGames-ec16}. The recommendation system is a ``principal" that interacts with self-interested ``agents" who arrive one by one. Each agent needs to make a decision: take an action from a given set of alternatives. The principal sends a \emph{message} to the agent according to some ``disclosure policy", \eg issues a recommendation. Then the agent chooses an action, and both the agent and the principal observe the outcome.  Crucially, the principal does not control the agent's decision. The problem is to design the disclosure policy for the principal that learns over time to issue messages so as to incentivize the agents to balance exploration and exploitation in a socially optimal way. 
A single round of this model is a version of a well-known ``Bayesian Persuasion game" \cite{Kamenica-aer11}.

\xhdr{Our scope.} Prior work on incentivizing exploration, with or without monetary incentives, achieves much progress (more on this in ``related work"), but relies heavily on the standard assumptions of Bayesian rationality and the ``power to commit" (\ie users trust the system to implement the mechanism that it claims to implement). However, these assumptions are quite problematic when the disclosure policy selectively withholds information, \eg recommends randomly chosen actions to some randomly selected agents (which is a major technique in the prior work). 

We take a step back and re-examine these assumptions. We ask: How would users interpret messages/recommendations generated by a disclosure policy? More importantly, how to design disclosure policies that would be interpreted by users in a predictable way, while ensuring good exploration-exploitation balance? Without the standard assumptions, which conceptual frameworks can be used to reason about the user response to a disclosure policy in precise terms?

We build one such conceptual framework for a particular type of disclosure policies. Consider the \emph{full-disclosure policy} that reveals the full history of observations from the previous users. We interpret it as the ``gold standard": we posit that users would trust such policy, even if they cannot verify it. Unfortunately, the full-disclosure policy is not good for our purposes, essentially because rational users would \emph{exploit} rather than \emph{explore}. However, for the sake of the argument, what if the policy reveals the history for every other agent, rather than the history for all agents? We posit that users would trust such policy, too. Given a large volume of data, we posit that users would not be too unhappy with having access to only a fraction this data. A crucial aspect of our intuition here is that the ``subhistory" revealed to a given user is chosen in advance, without looking at the observations. In particular, the disclosure policy cannot subsample the observations that make a particular action look good.

\ascomment{stopped for the day ...}




\xhdr{Related work.}


\cite{Perchet2015BatchedBP} gives regret guarantees for mult-armed bandit in $M$ batches. They have 2 upper bounds which are similar to our Theorem \ref{thm:llevel-1} and Theorem \ref{thm:llevel-2}.
