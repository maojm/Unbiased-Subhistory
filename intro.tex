\section{Introduction}
\label{sec:intro}

\input{intro-intro.tex}

\xhdr{Our scope.} Prior work on incentivizing exploration, with or without monetary incentives, achieves much progress (more on this in ``related work"), but relies heavily on the standard assumptions of Bayesian rationality and the ``power to commit" (\ie users trust that the principal actually implements the policy that it claims to implement). However, these assumptions appear quite problematic in the context of recommendation systems. In particular, much of the prior work suggests disclosure policies that merely recommend an action to each agent, without any other supporting information, and moreover recommend exploratory actions to some randomly selected users. This works out extremely well in theory, but it is very unclear whether users would trust the principal to implement the stated policy and, even if they do, whether they would react to it rationally. Several issues are in play: to wit, whether the principal intentionally uses a different disclosure policy than the claimed one (\eg because its incentives are not quite aligned with the users'), whether the principal correctly implements the policy that it wants to implement, whether the users trust the principal to make correct inferences on their behalf, and whether they find it acceptable that they may be singled out for exploration. Furthermore, regardless of how the users react to such disclosure policies, they may prefer not to be subjected to them, and leave the system.

We strive to design disclosure policies which mitigate these issues and (still) incentivize a good balance between exploration and exploitation. While some assumptions on human behavior are unavoidable, we are looking for a class of disclosure policies for which we can make plausible behavioral assumptions. Then we arrive at a concrete mathematical problem: design policies from this class so as to optimize performance, \ie  the induced explore-exploit tradeoff. Our goal in terms of performance is to approach the performance of the social planner.

\xhdr{Our model.}
For the sake of intuition, let us revisit the \emph{full-disclosure policy} that reveals the full history of observations from the previous users. We interpret it as the ``gold standard": we posit that users would trust such policy, even if they cannot verify it. Unfortunately, the full-disclosure policy is not good for our purposes, essentially because rational users would \emph{exploit} rather than \emph{explore}. However, what if a disclosure policy reveals the outcomes for every other agent, rather than the outcomes for all agents? We posit that users would trust such policy, too. Given a large volume of data, we posit that users would not be too unhappy with having access to only a fraction this data. A crucial aspect of our intuition here is that the ``subhistory" revealed to a given user comes from a subset of previous users that is chosen in advance, without looking at what happens during the execution. In particular, the subhistory is not ``biased", in the sense that the disclosure policy cannot subsample the observations in favor of a particular action.

With this intuition in mind, we define the class of \emph{unbiased-subhistory policies}: disclosure policies that reveal, to each arriving agent $t$, a subhistory  consisting of the outcomes for a subset $S_t$ of previous agents, where $S_t$ is chosen ahead of time. Further, we impose a transitivity property: if $t' \in S_t$, for some previous agent $t'$, then $S_{t'}\subset S_t$. So, agent $t$ has all information that agent $t'$ had at the time she chose her action. In particular, agent $t$ does not need to second-guess which message has caused agent $t'$ to make choose that action.

Following much of the prior work on incentivizing exploration, we do not attempt to model heterogenous agent preferences and non-stationarity. Formally, we assume that the expected reward of taking a given action $a$, denoted $\mu_a$, is the same for all agents, and does not change over time. Then the crucial parameter of interest, for a given action $a$, are the number of samples $N_a$ and the empirical mean reward $\bar{\mu}_a$ in the observed subhistory. We consider a flexible model of agent response: for each action $a$ an agent forms an estimate $\hat{\mu}_a$ of the mean reward $\mu_a$, roughly following $\bar{\mu}_a$ but taking into account the uncertainty due to a small number of samples, and chooses an action with a largest reward estimate. We allow the reward estimates to be arbitrary otherwise, and not known to the principal.

\xhdr{Regret.} We measure the performance of a disclosure policy in terms of \emph{regret}, a standard notion from the literature on multi-armed bandits. Regret is defined as the difference in the total expected reward between the best fixed action and actions induced by the policy. Regret is typically studied as a function of the time horizon $T$, which in our model is the number of agents. For multi-armed bandits, $o(T)$ regret bounds are deemed non-trivial, and $O(\sqrt{T})$ regret bounds are optimal in the worst case. Regret bounds that depend on a particular problem instance are also considered. A crucial parameter then is the \emph{gap} $\Delta$, the difference between the best and second best expected reward. One can achieve $O(\tfrac{1}{\Delta}\; \log T)$ regret rate, without knowing the $\Delta$.

\xhdr{Our results and discussion.}
Our main result is a transitive, unbiased-subhistory policy which attains near-optimal $\tilde{O}(\sqrt{T})$ regret rate for a constant number of actions. This policy also obtains the optimal instance-dependent regret rate
    $\tilde{O}(\tfrac{1}{\Delta})$
for problem instances with gap $\Delta$, without knowing the $\Delta$ in advance. In particular, we match the regret rate achieved for incentivizing exploration with unrestricted disclosure policies \cite{ICexploration-ec15-working}.

The main challenge is that the agents still follow exploitation-only behavior, just like they do for the full-disclosure policy, albeit based only on a portion of history. A disclosure policy controls the flow of information (who sees what), but not the \emph{content} of that information. 

The first step is to obtain any substantial improvement over the full-disclosure policy. We accomplish this with a relatively simple policy which runs the full-disclosure policy ``in parallel" on several disjoint subsets of agents,  collects all data from these runs and discloses it to all remaining agents. While any single run of the full-disclosure policy may get stuck on a suboptimal arm, having these parallel runs ensure that sufficiently many of them will ``get lucky" and provide some exploration. This simple policy achieves $\tilde{O}(T^{2/3})$ regret. Conceptually, it implements a basic bandit algorithm that explores uniformly for a pre-set number of rounds, then picks one arm for exploitation and stays with it for the remaining rounds. We think of this policy  as having two ``levels": Level 1 contains the parallel runs, and Level 2 is everything else. 

The next step is to implement \emph{adaptive exploration}, when exploration schedule is adapted to previous observations. This is needed to improve over the $\tilde{O}(T^{2/3})$ regret. We upgrade the simple two-level policy with a middle level, where agents receive the data collected in some (but not all) runs from the first level. Essentially, this new level provides exploration only if the gap $\Delta$ is small (and therefore more exploration is needed).

The main result extends this construction to multiple levels, connected in fairly intricate ways. \ascomment{add more}.



\ascomment{stable-ish text up to here.}


\input{related-work}

