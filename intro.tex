\section{Introduction}
\label{sec:intro}

\input{intro-intro.tex}

\xhdr{Our scope.} Prior work on incentivizing exploration, with or without monetary incentives, achieves much progress (more on this in ``related work"), but relies heavily on the standard assumptions of Bayesian rationality and the ``power to commit" (\ie users trust that the principal actually implements the policy that it claims to implement). However, these assumptions appear quite problematic in the context of recommendation systems. In particular, disclosure policies studied in prior work merely recommend an action to each agent, without any other supporting information, and moreover recommend exploratory actions to some randomly selected users. This works out extremely well in theory, but it is very unclear whether users would trust the principal to implement the stated policy and, even if they do, whether they would react to it rationally. Several issues are in play: to wit, whether the principal intentionally uses a different disclosure policy than the claimed one (\eg because its incentives are not quite aligned with the users'), whether the principal correctly implements the policy that it wants to implement, whether the users trust the principal to make correct inferences on their behalf, and whether they find it acceptable that they may be singled out for exploration. Furthermore, regardless of how the users react to such disclosure policies, they may prefer not to be subjected to them, and leave the system.

We strive to design disclosure policies which mitigate these issues and (still) incentivize a good balance between exploration and exploitation. While some assumptions on human behavior are unavoidable, we are looking for a class of disclosure policies for which we can make plausible behavioral assumptions. Then we arrive at a concrete mathematical problem: design policies from this class so as to optimize performance, \ie  the induced explore-exploit tradeoff. Our goal in terms of performance is as ambitious as the one in prior work on incentivizing exploration: essentially, to approach the performance of the social planner.

\ascomment{stable text up to here.}

\xhdr{Our contributions.}
We build one such conceptual framework for a particular type of disclosure policies. Consider the \emph{full-disclosure policy} that reveals the full history of observations from the previous users. We interpret it as the ``gold standard": we posit that users would trust such policy, even if they cannot verify it. Unfortunately, the full-disclosure policy is not good for our purposes, essentially because rational users would \emph{exploit} rather than \emph{explore}. However, for the sake of the argument, what if the policy reveals the history for every other agent, rather than the history for all agents? We posit that users would trust such policy, too. Given a large volume of data, we posit that users would not be too unhappy with having access to only a fraction this data. A crucial aspect of our intuition here is that the ``subhistory" revealed to a given user is chosen in advance, without looking at the observations. In particular, the disclosure policy cannot subsample the observations that make a particular action look good.






\xhdr{Related work.}


\cite{Perchet2015BatchedBP} gives regret guarantees for mult-armed bandit in $M$ batches. They have 2 upper bounds which are similar to our Theorem \ref{thm:llevel-1} and Theorem \ref{thm:llevel-2}.
