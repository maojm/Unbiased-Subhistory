%!TEX root = main.tex
\section{\ALGG: Revealing Full History}

\swedit{Before we describe our algorithms, we will first analyze a
  simple policy \ALGG \nicomment{as jieming and i discussed offline, this is a bad name for a policy; policies just determine the history and so aren't ``greedy'' in any sense i can see} that fully discloses the history over all
  previous rounds\nidelete{, and lets the agents make the ``greedy'' choices
  across all rounds}. Even though the algorithm does not guarantee low
  regret, it will be used as a subroutine in our main algorithms.  In
  particular, we show that because of the randomness in the rewards,
  with constant probability, \ALGG will induce the agents to
  play each arm at least once.} \nicomment{as we're talking about behavioral agents and not necessarily rational ones, i find it strange to talk about ``greedy'' agents -- which suggests to me that agents use an estimator equal to the empirical mean -- and to use the word ``incentivize'' (i changed it to ``induce'' in the previous paragraph).}

\begin{lemma}
\label{lem:greedy}
Under Assumption \ref{ass:embehave}, there exist two constants $\GdT = O_K(1)$
and $\GdP = \Omega_K(1)$ such that for any arm $a$, with probability
at least $\GdP$, \ALGG of $\GdT$ rounds pulls this arm at least
once.\footnote{The notations $O_k$ and $\Omega_K$ hide the dependence
  on the number of arms $K$.} \nicomment{If we change the name of the policy, we might also want to change the subscripts of these constants.}
\end{lemma}

\begin{proof}
  Fix any arm $a$. Let $\GdT = (K-1) \cdot c_T + 1$ and
  $\GdP = (1/3)^{\GdT}$. \swedit{We will condition on the event that
    all the realized rewards in $\GdT$ rounds are 0, which occurs with
    probability at least $\GdP$ under Assumption~\ref{ass:embehave}.}
  In this case, we want to show that arm $a$ is pulled at least
  once. We prove this by contradiction. Suppose arm $a$ is not pulled. By
  the pigeonhole principle, we know that there is some other arm $a'$
  that is pulled at least $c_T + 1$ rounds. Let $t$ be the round in
  which arm $a'$ is pulled exactly $c_T + 1$ times. By Assumption
  \ref{ass:embehave}, we know
  \[
    \hat{\mu}_{a'}^t \leq 0 + c_m / \sqrt{c_T} < 1/3. 
  \]
  \nicomment{Wait, where did we set $c_T$?} On the other hand, we have
  $\hat{\mu}_a^t \geq 1/3 > \hat{\mu}_{a'}^t$. This contradicts with
  the fact that in round $t$, arm $a'$ is pulled, instead of arm $a$.
  \swdelete{ In addition, we know that this case happens with
    probability at least $(1/3)^{\GdT} = \GdP$ as each arm's mean is
    at most $2/3$. To sum up, we know that with probability at least
    $\GdP$, \ALGG of $\GdT$ rounds pulls arm $a$ at least once.}
   \jmcomment{Agree with these changes.}
\end{proof}


\swedit{In our policies presented in the next sections, we will use
  \ALGG of $T_G$ rounds as a subroutine for initial exploration. For
  any $a$, let $q_a$ \nicomment{$q$ sounds like a fraction; maybe $n^G_a$ or $N^G_a$} be the expected number of arm $a$ pulls in one
  run of \ALGG of $T_G$ rounds.} \nicomment{\ALGG of $T_G$ rounds is a mouthful; what about using notation like \ALGG($T_G$)? or if we will always use $T_G$ rounds, then just define the policy to be that from now on?}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
