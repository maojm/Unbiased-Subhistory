%!TEX root = main.tex

\section{3-level Adaptive Policy}
\label{sec:3level}
\swcomment{rewrote below}
Now we demonstrate how to design a 3-level policy to incentivize
adaptive exploration and improve the $O(T^{2/3})$ regret rate to
$\tilde O(T^{4/7})$. In \Cref{sec:llevel}, we will show how to extend
this idea to achieve a regret rate of $\tilde O(T^{1/2})$. For
simplicity, we will focus on the setting with 2 arms.


The policy consists of three phases, each of which corresponds to a
level (see Figure~\ref{fig:3level}).  In the first level, the policy
performs the first round of exploration over the first
$(S\cdot T_1\cdot T_G)$ agents, where $T_1 = T^{4/7}\log^{-1/7}(T)$
and $S = 2^{10}\log(T)$. In particular, the policy randomly partitions
the agents into $S$ groups such that each group consists of $T_1$ runs
of \ALGG with $T_G$ rounds. In the second level, the policy performs
further exploration on the next $(S\cdot T_2)$ agents by incorporating
data collected in the first level, where
$T_2 = T^{6/7}\log^{-5/7}(T)$. The agents are again partitioned into
$S$ groups such that the agents in the $s$-th group are shown the same
sub-history from the $s$-th group in the first level. In the third
level, the policy exploits by showing all remaining agents the entire
history from the first two levels.


They main idea for the 3-level policy is that it inserts the second
level as an additional ``checkpoint'' to induce more adaptivity in the
exploration. While the first level ensures that the agents explore
both arms, the second level will continue to explore both arms only if
the rewards of the two arms are still indistinguishable (that is, when
$|\mu_1 - \mu_2| < \tilde{O}(1/\sqrt{T_1})$). In the third level,
agents will pull the best arm unless $|\mu_1-\mu_2|$ is much smaller
than $\tilde{O}(1/\sqrt{T_2})$, in which case pulling either arm will
incur small regret. The formal guarantee of the policy is the
following.

\begin{theorem}
\label{thm:3level}
The 3-level recommendation policy gets expected regret $O(T^{4/7} \log^{6/7}(T))$. 
\end{theorem}



In the proof, we need not only the concentration argument as the
2-level recommendation policy, but also the anti-concentration
argument to show that agents explore more in the second round if
$\mu_1-\mu_2$ is not too big.


\iffalse
Set $T_1 = T^{4/7}\log^{-1/7}(T)$ and $T_2 =
T^{6/7}\log^{-5/7}(T)$. The first level has $S = 2^{10}\log(T)$ groups
of agents. Each group runs $T_1$ \ALGG of $T_G$ rounds in
parallel. The second level also has $S$ groups of agents. Each group
has $T_2$ agents and they observe the history of corresponding group
in the first level. All the rest agents are in the third level and
they observe the entire history of the first two levels.  See also
Figure \ref{fig:3level} for a graphical view of the information flow.



Here we give some intuition about how the 3-level recommendation
policy works. The main idea is to do the exploration more
adaptively. In the first level, agents explore both arms. In the
second level, agents will pull the best arm when $\mu_1 - \mu_2$ is
large ($\tilde{\Omega}(1/\sqrt{T_1})$). Otherwise agents will explore
both arms more. In the third level, agents will pull the best arm
unless $\mu_1-\mu_2$ is much smaller ($\tilde{O}(1/\sqrt{T_2})$). In
the proof, we need not only the concentration argument as the 2-level
recommendation policy, but also the anti-concentration argument to
show that agents explore more in the second round if $\mu_1-\mu_2$ is
not too big.
\fi


\begin{figure}[H]
\centering
\begin{tikzpicture}  
 \filldraw[fill=green!20!white]
 (0,4)--(10,4)--(10,5)--(0,5)--cycle;
 \foreach \x in {0,3,8}
 {
 \filldraw[fill=blue!20!white]
 (\x+0,2)--(\x+2,2)--(\x+2,3)--(\x+0,3)--cycle;
 \draw[dashed] (\x+1,3)--(5,4);
 \filldraw[fill=red!20!white]
 (\x+0,0)--(\x+2,0)--(\x+2,1)--(\x+0,1)--cycle;
 \draw[dashed] (\x+1,1)--(\x+1,2);
 \draw[dashed] (\x+0.2,1)--(\x+1,2);
 \draw[dashed] (\x+0.6,1)--(\x+1,2);
 \draw[dashed] (\x+1.4,1)--(\x+1,2);
 \draw[dashed] (\x+1.8,1)--(\x+1,2);
 \draw[dashed] (\x+0.4,0)--(\x+0.4,1); 
 \draw[dashed] (\x+0.8,0)--(\x+0.8,1); 
 \draw[dashed] (\x+1.2,0)--(\x+1.2,1); 
 \draw[dashed] (\x+1.6,0)--(\x+1.6,1);  
 \node at(\x+1,2.5){$T_2$};
 \node at(\x+0.2, 0.5){$\GdT$};
 \node at(\x+0.6, 0.5){$\cdot$};
 \node at(\x+1.0, 0.5){$\cdot$};
 \node at(\x+1.4, 0.5){$\cdot$};
 \node at(\x+1.8, 0.5){$\GdT$};
 %\node at(\x+1,0.5){$T_1 \cdot \GdT$}; 
 \draw [decorate,decoration={brace,amplitude=10pt},xshift=0pt,yshift=0pt] (\x+2,-0.2) -- (\x+0,-0.2) node [black,midway,yshift=-0.6cm] {$T_1$ runs};
 }
  \node at(5,4.5){$T -S(T_1 \cdot \GdT + T_2)$};
  \node at (6,0.5){$\cdots$};
  \node at (7,0.5){$\cdots$};
  \node at (6,2.5){$\cdots$};
  \node at (7,2.5){$\cdots$};
  \node at(-1,0.5){\textbf{Level 1}};
  \node at(-1,2.5){\textbf{Level 2}};
  \node at(-1,4.5){\textbf{Levle 3}};
  \draw[->] (11,0)--(11,5);
  \node at(11.5,2.5)[ rotate=90]{Time};
  
  \draw [decorate,decoration={brace,amplitude=10pt,aspect=0.33},xshift=0pt,yshift=0pt] (10,1.8) -- (0,1.8) node [black,pos=0.33,xshift = 0cm,yshift=-0.6cm] {$S$ groups};
  
\end{tikzpicture}
\caption{Structure of the information graph for the 3-level
  Policy. Each red box in level 1 corresponds to $T_1$ \ALGG paths of
  length $T_G$. The sub-history of each red box is then aggregated and
  sent to a corresponding purple box of agents in level 2. Then the
  entire history of levels 1 and 2 is aggregated and shown to all
  agents in level 3.}
\label{fig:3level}
\end{figure}
\swcomment{check the figure caption above. can we index the indicate
  there are $S$ groups in the figure?}


\begin{lemma}[``Clean'' events]

\end{lemma}





\begin{proof}
Wlog we assume $\mu_1 \geq \mu_2$ as the recommendation policy is symmetric to both arms. We do a case analysis based on $\mu_1-\mu_2$. 

Before we start with the case analysis, we first define several clean events and show that the intersection of them happens with high probability. 
\begin{itemize}
\item \textbf{Concentration of the number of arm $a$ pulls in the first level:} 
By Lemma \ref{lem:greedy}, we know $\GdP \leq q_a \leq \GdT$. For the $s$-th first-level group, define $W_1^{a,s}$ to be the event that the number of arm $a$ pulls in the $s$-th first-level group is between $q_a T_1- \GdT \sqrt{T_1\log(T)}$ and $q_a T_1 + \GdT \sqrt{T_1\log(T)}$. By Chernoff bound,
\[
\Pr[W_1^{a,s}] \geq 1-2\exp(-2\log(T)) \geq 1-2/T^2.
\]
Define $W_1$ to be the intersection of all these events (i.e. $W_1 = \bigcap_{a,s}W_1^{a,s}$). By union bound, we have
\[
\Pr[W_1] \geq 1- \frac{4S}{T^2}.
\]
\item \textbf{Concentration of the empirical mean of arm $a$ pulls in the first level:}
For each first-level group and arm $a$, imagine there is a tape of enough arm $a$ pulls sampled before the recommendation policy starts and these samples are revealed one by one whenever agents in this group pull arm $a$. For the $s$-th first-level group and arm $a$, define $W_2^{s,a,t_1,t_2}$ to be the event that the mean of $t_1$-th to $t_2$-th pulls in the tape is at most $\sqrt{\frac{2\log(T)}{t_2-t_1+1}}$ away from $\mu_a$. By Chernoff bound,\swcomment{a bit confused about what $t_1$ and $t_2$ mean?}
\[
\Pr[W_2^{s,a,t_1,t_2}] \geq 1 - 2\exp(-4\log(T)) \geq 1- 2/T^4.
\]

Define $W_2$ to be the intersection of all these events (i.e. $W_2 = \bigcap_{a,s,t_1,t_2} W_2^{s,a,t_1,t_2}$). By union bound, we have
\[
\Pr[W_2] \geq 1- \frac{4S}{T^2}.
\]

\item \textbf{Concentration of the empirical mean of arm $a$ pulls in the first two levels:}

For all the groups in the first two levels and arm $a$, imagine there is a tape of enough arm $a$ pulls sampled before the recommendation policy starts and these samples are revealed one by one whenever agents in the first two levels pull arm $a$. Define $W_3^{a,t}$ to be the event that the mean of the first $t$ pulls in the tape is at most $\sqrt{\frac{2\log(T)}{t}}$ away from $\mu_a$. By Chernoff bound, 
\[
\Pr[W_3^{a,t}] \geq 1 - 2\exp(-4\log(T)) \geq 1- 2/T^4.
\]
Define $W_3$ to be the intersection of all these events (i.e. $W_3 = \bigcap_{a,t} W_3^{a,t}$). By union bound, we have
\[
\Pr[W_3] \geq 1- \frac{4}{T^3}.
\]

\item \textbf{Anti-concentration of the empirical mean of arm $a$ pulls in the first level:}

Consider the tapes defined in the second bullet again. For the $s$-th first-level group and arm $a$, define $W_4^{s,a,high}$  to be the event that first $q_a T_1$ pulls of arm $a$ in the corresponding tape has empirical mean at least $\mu_a + 1/\sqrt{q_a T_1}$ and define  $W_4^{s,a,low}$  to be the event that first $q_a T_1$ pulls of arm $a$ in the corresponding tape has empirical mean at most $\mu_a - 1/\sqrt{q_a T_1}$. By Berry-Essen Theorem and $\mu_a \in [1/3,2/3]$, we have
\[
\Pr[W_4^{s,a,high}] \geq (1-\Phi(1/2)) - \frac{5}{\sqrt{q_aT_1}} > 1/4.
\]
The last inequality follows when $T$ is larger than some constant.
Similarly we also have 
\[
\Pr[W_4^{s,a,low}] > 1/4.
\]
Since $W_4^{s,a,high}$ is independent with $W_4^{s,3-a,low}$, we have
\[
\Pr[W_4^{s,a,high} \cap W_4^{s,3-a,low}] =\Pr[W_4^{s,a,high}] \cdot  \Pr[W_4^{s,3-a,low}]>(1/4)^2 = 1/16.
\]
Now define $W_4$ as $\bigcap_a \bigcup_s (W_4^{s,a,high} \cap W_4^{s,3-a,low})$. Notice that $(W_4^{s,a,high} \cap W_4^{s,3-a,low})$ are independent across different $s$'s. By union bound, we have
\[
\Pr[W_4] \geq 1- 2(1-1/16)^S \geq 1 -2 /T.
\]
\end{itemize}

By union bound, the intersection of these clean events (i.e. $\bigcap_{i=1}^4 W_i$) happens with probability $1-O(1/T)$. When this intersection does not happen, since the probability is $O(1/T)$, it contributes $O(1/T) \cdot T = O(1)$ to the expected regret. 

Now we assume the intersection of clean events happens and we summarize what these clean events imply.

\begin{itemize}
\item For the $s$-th first-level group and arm $a$, define $\bar{\mu}_a^{1,s}$ to be the empirical mean of arm $a$ pulls in this group. $W_1^{a,s}$, $W_2^{a,s,1,t}$ for $ = q_a T_1- \GdT \sqrt{T_1\log(T)},...,q_a T_1- \GdT \sqrt{T_1\log(T)}$ together imply that
\[
|\bar{\mu}_a^{1,s} - \mu_a| \leq \sqrt{\frac{2\log(T)}{q_a T_1- \GdT \sqrt{T_1\log(T)}}} \leq \sqrt{\frac{4\log(T)}{q_a T_1}}.
\]
The last inequality holds when $T$ is larger than some constant.
\item For each arm $a$, define $\bar{\mu}_a$ to be the empirical mean of arm $a$ pulls in the first two levels. $W_1^{a,s}$ for $s=1,...,S$ and $W_3^{a,t}$ for $t \geq  (q_a T_1- \GdT \sqrt{T_1\log(T)})S$ together imply that
\[
|\bar{\mu}_a - \mu_a| \leq \sqrt{\frac{2\log(T)}{S\left(q_a T_1- \GdT \sqrt{T_1\log(T)}\right)}} \leq \sqrt{\frac{4\log(T)}{S q_a T_1}} .
\]
The last inequality holds when $T$ is larger than some constant.

If there are at least $T_2$ pulls of arm $a$ in the first two levels, 
\[
|\bar{\mu}_a-\mu_a| \leq \sqrt{\frac{2\log(T)}{T_2}}. 
\]

\item For each $a \in \{1,2\}$, $W_4$ implies that there exists $s_a$ such that $W_4^{s_a,a,high}$ and $W_4^{s_a,3-a,low}$ happen. $W_4^{s_a,a,high}$,  $W_1^{s_a,a}$, $W_2^{s_a,a,t, q_aT_1}$ for $t = q_a T_1- \GdT \sqrt{T_1\log(T)}+1, ...,q_aT_1-1$ and $W_2^{s_a,a,q_aT_1,t}$ for $t= q_aT_1,...,q_a T_1+ \GdT \sqrt{T_1\log(T)}$ together imply that 
\begin{align*}
\bar{\mu}_a ^{1,s_a} &\geq \mu_a + \left(q_aT_1 \cdot \frac{1}{\sqrt{q_aT_1}} - \GdT \sqrt{T_1\log(T)} \cdot \sqrt{\frac{2\log(T)}{ \GdT \sqrt{T_1\log(T)}}} \right) \cdot \frac{1}{q_a T_1+ \GdT \sqrt{T_1\log(T)}} \\
&> \mu_a + \frac{1}{4\sqrt{q_aT_1}}.
\end{align*}
The second last inequality holds when $T$ is larger than some constant.
Similarly, we also have
\[
\bar{\mu}_{3-a} ^{1,s_a} < \mu_{3-a}   - \frac{1}{4\sqrt{q_{3-a} T_1}}.
\]
\end{itemize}

Finally we proceed to the case analysis. We give upper bounds on the expected regret conditioned on the intersection of clean events.

\begin{itemize}
\item $\mu_1 - \mu_2 \geq 2\left(\sqrt{\frac{4\log(T)}{q_1T_1}} 
+ \sqrt{\frac{4\log(T)}{q_2T_1}}\right)$. In this case, we want to show that agents in the second and the third levels all pull arm 1. 

First consider the $s$-th second-level group. We know that 
\[
\bar{\mu}_1^{1,s} - \bar{\mu}_2^{1,s} \geq \mu_1 -\mu_2 - \sqrt{\frac{4\log(T)}{q_1T_1}} - \sqrt{\frac{4\log(T)}{q_2T_1}} \geq  \sqrt{\frac{4\log(T)}{q_1T_1}} + \sqrt{\frac{4\log(T)}{q_2T_1}}.
\]
For any agent $t$ in the $s$-th second-level group, by Assumption \ref{ass:embehave}, we have
\begin{align*}
\hat{\mu}_1^t - \hat{\mu}_2^t &>\bar{\mu}_1^{1,s} - \bar{\mu}_2^{1,s} - \frac{c_m}{\sqrt{q_1T_1/2}} - \frac{c_m}{\sqrt{q_2T_1/2}}\\
&\geq  \sqrt{\frac{4\log(T)}{q_1T_1}} + \sqrt{\frac{4\log(T)}{q_2T_1}}- \frac{c_m}{\sqrt{q_1T_1/2}} - \frac{c_m}{\sqrt{q_2T_1/2}}\\
 &> 0.
\end{align*}
Therefore, we know agents in the $s$-th second-level group will all pull arm 1.

Now consider the agents in the third level group. Recall $\bar{\mu}_a$ is the empirical mean of arm $a$ in the history they see. We have
\[
\bar{\mu}_1 - \bar{\mu}_2 \geq \mu_1 -\mu_2 - \sqrt{\frac{4\log(T)}{Sq_1T_1}} - \sqrt{\frac{4\log(T)}{Sq_2T_1}} \geq  \sqrt{\frac{4\log(T)}{q_1T_1}} 
+ \sqrt{\frac{4\log(T)}{q_2T_1}}.
\]
Similarly as above, by Assumption \ref{ass:embehave}, we know $\hat{\mu}_1^t - \hat{\mu}_2^t > 0$ for any agent $t$ in the third level. So we know agents in the third-level group will all pull arm 1. Therefore the expected regret is at most $S T_G T_1 = O(T^{4/7} \log^{6/7}(T))$. 


\item $2\left(\sqrt{\frac{4\log(T)}{Sq_1T_1}} 
+ \sqrt{\frac{4\log(T)}{Sq_2T_1}}\right) \leq \mu_1-\mu_2 < 2\left(\sqrt{\frac{4\log(T)}{q_1T_1}} 
+ \sqrt{\frac{4\log(T)}{q_2T_1}}\right)$. In this case, we want to show agents in the third level all pull arm 1. Recall $\bar{\mu}_a$ is the empirical mean of arm $a$ in the first two levels. We have
\[
\bar{\mu}_1 - \bar{\mu}_2 \geq \mu_1 -\mu_2 - \sqrt{\frac{4\log(T)}{Sq_1T_1}} - \sqrt{\frac{4\log(T)}{Sq_2T_1}} \geq  \sqrt{\frac{4\log(T)}{Sq_1T_1}} 
+ \sqrt{\frac{4\log(T)}{Sq_2T_1}}.
\]
For any agent $t$ in the third level, by Assumption \ref{ass:embehave}, we have
\begin{align*}
\hat{\mu}_1^t - \hat{\mu}_2^t &>\bar{\mu}_1 - \bar{\mu}_2 - \frac{c_m}{\sqrt{Sq_1T_1/2}} - \frac{c_m}{\sqrt{Sq_2T_1/2}}\\
&\geq  \sqrt{\frac{4\log(T)}{Sq_1T_1}} + \sqrt{\frac{4\log(T)}{Sq_2T_1}}- \frac{c_m}{\sqrt{Sq_1T_1/2}} - \frac{c_m}{\sqrt{Sq_2T_1/2}}\\
 &> 0.
\end{align*}
So we know agents in the third-level group will all pull arm 1. Therefore the expected regret is at most 
\[
(S T_G T_1 + S T_2) \cdot 2\left(\sqrt{\frac{4\log(T)}{q_1T_1}} 
+ \sqrt{\frac{4\log(T)}{q_2T_1}}\right) = O(T^{4/7} \log^{6/7}(T))
\]

\item $ 3\sqrt{\frac{2\log(T)}{T_2}} < \mu_1-\mu_2 < 2\left(\sqrt{\frac{4\log(T)}{Sq_1T_1}} 
+ \sqrt{\frac{4\log(T)}{Sq_2T_1}}\right)$. In this case, we just need to make sure that agents in the third level all pull arm 1. To do so, we need both arms to be pulled at least $T_2$ rounds in the second level.  

Now consider the $s_a$-th second-level group. We have
\begin{align*}
\bar{\mu}_a^{1,s_a} - \bar{\mu}_{3-a}^{1,s_a} &> \mu_a + \frac{1}{4\sqrt{q_aT_1}} -\mu_{3-a} +\frac{1}{4\sqrt{q_{3-a}T_1}} \\
&> \frac{1}{4\sqrt{q_1T_1}}+ \frac{1}{4\sqrt{q_2T_1}} - 2\left(\sqrt{\frac{4\log(T)}{Sq_1T_1}} 
+ \sqrt{\frac{4\log(T)}{Sq_2T_1}}\right) \\
&\geq \frac{1}{8\sqrt{q_1T_1}}+ \frac{1}{8\sqrt{q_2T_1}}.
\end{align*}
For any agent $t$ in the $s_a$-th second-level group, by Assumption \ref{ass:embehave}, we have
\begin{align*}
\hat{\mu}_a^t - \hat{\mu}_{3-a}^t &>\bar{\mu}_a^{1,s_a} - \bar{\mu}_{3-a}^{1,s_a} - \frac{c_m}{\sqrt{q_1T_1/2}} - \frac{c_m}{\sqrt{q_2T_1/2}}\\
&\geq   \frac{1}{8\sqrt{q_1T_1}}+ \frac{1}{8\sqrt{q_2T_1}}- \frac{c_m}{\sqrt{q_1T_1/2}} - \frac{c_m}{\sqrt{q_2T_1/2}}\\
 &> 0.
\end{align*}
So we know agents in the $s_a$-th second-level group will all pull arm $a$. Therefore in the first two levels, both arms are pulled at least $T_2$ times. Now consider the third-level. We have
\[
\bar{\mu}_1 - \bar{\mu}_2  \geq \mu_1 -\mu_2 - 2\sqrt{\frac{2\log(T)}{T_2}} \geq \sqrt{\frac{2\log(T)}{T_2}}.
\]
Similarly as above, by Assumption \ref{ass:embehave}, we know $\hat{\mu}_1^t - \hat{\mu}_2^t > 0$ for any agent $t$ in the third level. So we know agents in the third-level group will all pull arm 1.

Therefore the expected regret is at most 
\[
(S T_G T_1 + S T_2) \cdot 2\left(\sqrt{\frac{4\log(T)}{Sq_1T_1}} 
+ \sqrt{\frac{4\log(T)}{Sq_2T_1}}\right) \leq O(T^{4/7} \log^{6/7}(T))
\]


\item $\mu_1 - \mu_2 \leq 3\sqrt{\frac{2\log(T)}{T_2}}$. This is the easy case. Even always pulling the sub-optimal arm (i.e. arm 2) gives regret at most $T \cdot (\mu_1-\mu_2) = O(T^{4/7} \log^{6/7}(T))$. 
\end{itemize}

\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
