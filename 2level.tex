%!TEX root = main.tex
\section{Two-level Recommendation Policy}
Run $T_2$ \ALGG in parallel and then show the later agents the history of first $T_2$ rounds.

\begin{lemma}
For each arm $i$, any $t > 0$ and $\varepsilon \leq 1/2$, let $p$ be the probability that there exists $t \leq \tau \leq T$ such that the empirical mean of the first $\tau$ pulls of arm $i$ is $\varepsilon$ away from $\mu_i$. Then we have 
\[
p \leq \frac{2}{\varepsilon^2} \exp(-2\varepsilon^2 t).
\]
\end{lemma}

\begin{proof}
For each $\tau \in \{t,...,T\}$, let $p_{\tau}$ be the probability that the empirical mean of the first $\tau$ pulls of arm $i$ is $\varepsilon$ away from $\mu_i$. By Chernoff bound, we have
\[
p_{\tau} \leq 2 \exp(-2\varepsilon^2 \tau).
\]

By union bound,
\[
p \leq \sum_{\tau = t}^T p_{\tau} \leq 2 \exp(-2\varepsilon^2 t)  \cdot \frac{1}{1-\exp(-2\varepsilon^2)}.
\]

We know that for any $ x \in [0,1]$, $e^{-x} \leq 1-x/2$. There we have
\[
p \leq  2 \exp(-2\varepsilon^2 t)  \cdot \frac{1}{1-(1-\varepsilon^2)} =  \frac{2}{\varepsilon^2} \exp(-2\varepsilon^2 t).
\]
\end{proof}

\begin{theorem}
\2LEVEL gets expected regret $O(T^{2/3} \log(T)^{1/3})$. If $\delta$, the expected regret is $O(\log(T) \cdot \delta^{-2})$. 
\end{theorem}

\begin{proof}
We will analyze the case when agents after \ALGG only uses empirical mean.
\end{proof}