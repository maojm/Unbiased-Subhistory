%!TEX root = main.tex
\section{Warm-up: full-disclosure paths}

We first consider a disclosure policy that reveals the full history in each round $t$, \ie $m_t=\SubH{t-1}$; we call it the \emph{full-disclosure policy}. The info-path for this policy is a simple path. We use this policy as a ``gadget" in our constructions. Hence, we formulate it slightly more generally:
  
\begin{definition}
A subset of rounds $S\subset [T]$ is called a \emph{full-disclosure path} in the  info-graph $G$ if the induced subgraph $G_S$ is a simple path, and it connects to the rest of the graph only through the terminal node $\max(S)$, if at all.
\end{definition}
  
We prove that for a constant number of arms, with constant probability, a full-disclosure path of constant length suffices to sample each arm at least once. We will build on this fact throughout.

\begin{lemma}\label{lem:greedy}
There exist numbers $\GdT>0$ and $\GdP>0$ that depend only on $K$, the number of arms, with the following property. Consider an arbitrary disclosure policy, and let $S\subset [T]$ be a full-disclosure path in its info-graph, of length $|S|\geq \GdT$. Under Assumption \ref{ass:embehave}, with probability at least $\GdP$, subhistory $\SubH{S}$ contains at least once sample of each arm $a$.
\end{lemma}

\begin{proof}[Proof Sketch]
\end{proof}

\ascomment{Where do we need to introduce the expected \#pulls in full-disclosure path?}

We provide a simple disclosure policy based on full-disclosure paths. The policy follows the ``explore-then-exploit'' paradigm. The ``exploration phase" comprises the first $N = T_1\cdot \GdT$ rounds, and consists of $T_1$ full-disclosure paths of length $\GdT$ each. Here $\GdT$ is the constant from Lemma~\ref{lem:greedy}, and $T_1$ is a parameter. In the ``exploitation phase", each agent $t>N$ receives the full subhistory from exploration, \ie $m_t = \SubH{[N]}$. The info-graph for this disclosure policy is shown in Figure~\ref{fig:2level}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
 \filldraw[fill=blue!20!white]
 (0,2)--(10,2)--(10,3)--(0,3)--cycle;
  \filldraw[fill=red!20!white]
  (0,0)--(1,0)--(1,1)--(0,1)--cycle;
  \draw (0.5,1)--(5,2);
  \filldraw[fill=red!20!white]
  (1,0)--(2,0)--(2,1)--(1,1)--cycle;
  \draw (1.5,1)--(5,2);
  \filldraw[fill=red!20!white]
  (2,0)--(3,0)--(3,1)--(2,1)--cycle;
  \draw(2.5,1)--(5,2);
  \filldraw[fill=red!20!white]
  (3,0)--(4,0)--(4,1)--(3,1)--cycle;
  \draw(3.5,1)--(5,2);
  \filldraw[fill=red!20!white]
  (9,0)--(10,0)--(10,1)--(9,1)--cycle;
  \draw(9.5,1)--(5,2);
  \node at(5,0.5){$\cdots$};
  \node at(6,0.5){$\cdots$};
  \node at(7,0.5){$\cdots$};
  \node at(8,0.5){$\cdots$};
  \node at(5,2.5){$T -T_1 \cdot \GdT$ rounds};
  \node at(0.5,0.5){$\GdT$};
  \node at(1.5,0.5){$\GdT$};
  \node at(2.5,0.5){$\GdT$};
  \node at(3.5,0.5){$\GdT$};
  \node at(9.5,0.5){$\GdT$};
  \node at(-1,0.5){\textbf{Level 1}};
  \node at(-1,2.5){\textbf{Level 2}};
  \draw[->] (11,0)--(11,3);
  \node at(11.5,1.5)[ rotate=90]{Time};

  \draw [decorate,decoration={brace,amplitude=10pt},xshift=0pt,yshift=0pt] (10,-0.2) -- (0,-0.2) node [black,midway,yshift=-0.6cm] 
  {$T_1$ full-disclosure paths of length $\GdT$ each};
\end{tikzpicture}

\caption{Info-graph for the 2-level policy. }
%Each red box in level 1 corresponds to a path connecting a set of
%  $T_G$ agents. The entire history in level 1 is then aggregated and
%  shown to each agent in level 2.}
\label{fig:2level}
\end{figure}


It is useful to think of the info-graph as having two levels (corresponding to exploration and exploitation). Accordingly, we call this policy the \emph{2-level policy}. We show that it incentivizes the agents to perform non-adaptive exploration, and achieves a regret rate of  $\tilde O_K(T^{2/3})$. The key idea is that using many full-disclosure paths ``in parallel" ensures that sufficiently many samples of each arm are collected during exploration.

\begin{theorem}\label{thm:2level}
The 2-level policy with parameter $T_1 = T^{2/3}\,\log(T)^{1/3}$ achieves regret
\[ \reg(T) \leq O_K\left( T^{2/3}\, \log(T)^{1/3} \right).\]
\end{theorem}

\begin{remark}
For a constant $K$, the number of arms, we match the optimal regret rate for non-adaptive multi-armed bandit algorithms. If the gap parameter $\Delta$ is known to the principal, then (for an appropriate tuning of parameter $T_1$) we can achieve regret 
  $\reg(T) \leq O_K(\log(T) \cdot \Delta^{-2})$.
\end{remark}


\begin{lemma}[Concentration of arm pulls]\label{lem:t1runs}
  Consider $T_1$ indepdent runs of \ALGG of $T_G$ rounds. Let $N_a$ be
  the number of arm $a$ pulls among all runs. Then with probability at
  least $1-\delta$, for all $a\in \A$,
  \[
    \left| N_a - q_a T_1\right| \leq T_G \sqrt{T_1 \log(2K/\delta) / 2}
  \]
\end{lemma}






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
