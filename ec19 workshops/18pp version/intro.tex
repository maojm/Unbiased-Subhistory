\section{Introduction}
\label{sec:intro}

In the classic literature on multi-armed bandits, an agent repeatedly selects one of a set of actions, each of which has a payoff drawn from an unknown fixed distribution.  Over time, she can trade off {\em exploitation}, in which she picks an action to maximize her expected reward, with {\em exploration}, in which she takes potentially sub-optimal actions to learn more about their rewards.  By coordinating her actions across time, she can guarantee an average reward which converges to that of the optimal action in hindsight at a rate proportional to the inverse square-root of the time horizon.

In many decision problems of interest, the actions are not chosen by a single agent, as above, but rather a sequence of agents.  This is particularly common in social learning settings such as online websites, where a population of users try to learn about the content of the site.  In such settings, each agent will choose an exploitive action as the benefits of explorative actions are only accrued by future agents.  For example, in online retail, products are purchased by a sequence of customers, each of which buys what she estimates to be the best available product.  This behavior can cause herding, in which all agents eventually take a sub-optimal action of maximum expected payoff given the available information.

This situation can be circumvented by a centralized algorithm that induces agents to take explorative actions, an idea called {\em incentivizing exploration}.  Such algorithms are often encountered in the form of recommendations and are quite common in practice.  Many online websites, like Amazon, Reddit, Yelp, and Tripadvisor, among many others, use recommendation policies of some sort to help users navigate their offerings.
%
One way recommendation policies induce exploration is to introduce payments, \eg~\cite{Frazier-ec14,KKMPRVW17,Kempe-colt18}. For example, the recommendation system of an online retailer might offer coupons to agents for trying certain products.  When payments are financially or technologically infeasible, another alternative is to rely on information asymmetry, \eg~\cite{Kremer-JPE14,Che-13,ICexploration-ec15,Bimpikis-exploration-ms17}.
Here the idea is that the centralized algorithm, often called a {\em disclosure policy}, can choose to selectively release information about the past actions and rewards to the agents in the form of a {\em message}.  For example, the recommendation system of an online retailer might disclose past reviews or product rankings to the agents.  Importantly, agents can not directly observe the past, but only learn about it through this message.  The agent then chooses an action, using the content of the message as input.

\xhdr{Our scope.} Prior work on incentivizing exploration, with or without monetary incentives, achieves much progress (more on this in ``related work"), but relies heavily on the standard assumptions of Bayesian rationality and the ``power to commit" (\ie users trust that the principal actually implements the policy that it claims to implement). However, these assumptions appear quite problematic in the context of recommendation systems of actual online websites such as those mentioned above. In particular, much of the prior work suggests disclosure policies that merely recommend an action to each user, without any other supporting information, and \asedit{sometimes recommend exploratory actions}. This works out extremely well in theory, but it is very unclear whether users would know and  understand some complicated policy deployed by the principal, let alone trust the principal to implement this policy. Even if they do understand the policy and trust that it was implemented as claimed, it's unclear whether users would react to it rationally.
%
Several issues are in play: to wit, whether the principal intentionally uses a different disclosure policy than the claimed one (\eg because its incentives are not quite aligned with the users'), whether the principal correctly implements the policy that it wants to implement,%
\footnote{\asedit{Large-scale systems that implement bandit algorithms can be notoriously difficult to debug \cite{DS-arxiv}.}}
 whether the users trust the principal to make correct inferences on their behalf, and whether they find it acceptable that they may be singled out for exploration. Regardless of how the users react to such disclosure policies, they may prefer not to be subjected to them, and leave the system.

We strive to design disclosure policies which mitigate these issues and (still) incentivize a good balance between exploration and exploitation. While some assumptions on human behavior are unavoidable, we are looking for a class of disclosure policies for which \asedit{they are (more) plausible}. Then we arrive at a concrete mathematical problem: design policies from this class so as to optimize performance, \ie  the induced explore-exploit tradeoff. Our goal in terms of performance is to approach the performance of the social planner.

\xhdr{Our model.}
For the sake of intuition, let us revisit the \emph{full-disclosure policy} that reveals the full history of observations from the previous users. We interpret it as the ``gold standard": we posit that users would trust such policy, even if they cannot verify it. Unfortunately, the full-disclosure policy is not good for our purposes, essentially because we expect users to \emph{exploit} rather than \emph{explore}. However, what if a disclosure policy reveals the outcomes for every tenth user, rather than the outcomes for all users? We posit that users would trust such policy, too. Given a large volume of data, users would not be too unhappy with having access to only a fraction this data.

A crucial aspect of our intuition here is that the ``subhistory" revealed to a given user comes from a subset of previous users that is chosen in advance, without looking at what happens during the execution. In particular, the subhistory is not ``biased", in the sense that the disclosure policy cannot subsample the observations so as to make a particular action look good. \asedit{(If a subhistory is subsampled in favor of some action, revealing is essentially no better than just issuing a recommendation without any supporting evidence.)}
With this intuition in mind, we define the class of \emph{unbiased-subhistory policies}: disclosure policies that reveal, to each arriving agent $t$, a subhistory  consisting of the outcomes for a subset $S_t$ of previous agents, where $S_t$ is chosen ahead of time.
\asedit{(From here on, we will use ``users" and ``agents" interchangeably.)}

Further, we impose a transitivity property: if $t' \in S_t$, for some previous agent $t'$, then $S_{t'}\subset S_t$. In words, agent $t$ has all information that agent $t'$ had at the time she chose her action. \asedit{Agent $t$ does not need to second-guess which message has caused agent $t'$ to choose that action, since such second-guessing cannot possibly yield any additional information. Essentially, agent $t$ can treat the subhistory as a set of data points collected by an algorithm, and not worry how these data points has been affected by other agents' choices.}

Following much of the prior work on incentivizing exploration, we do not attempt to model heterogenous agent preferences and non-stationarity. We assume that the expected reward of taking a given action $a$, denoted $\mu_a$, is the same for all agents, and does not change over time.

We consider a flexible model of agent response. For each action $a$, the crucial statistics are the number of samples $N_a$ and the empirical mean reward $\bar{\mu}_a$ in the observed subhistory. An agent forms an estimate $\hat{\mu}_a$ of the mean reward $\mu_a$, roughly following $\bar{\mu}_a$ but taking into account the uncertainty due to a small number of samples, and chooses an action with a largest reward estimate. We allow the reward estimates to be arbitrary otherwise.
\asedit{The way they depend on the observed statistics may differ from one agent to another. This dependence, as well as the statistics and the reward estimates, are not known to the principal. We make no assumptions whatsoever unless an agent has sufficiently many samples from a given arm.}

\asedit{Our model of agent response is justified by the assumption of unbiased, transitive subhistory. This assumption is no longer needed once the model is assumed. The model is justified regardless of what the agents know about the subsets $(S_{t}: t\in [T])$. In particular, we could posit that these subsets are common knowledge, or that the agents do not know them, not even their own subset.}

\xhdr{Regret.} We measure the performance of a disclosure policy in terms of \emph{regret}, a standard notion from the literature on multi-armed bandits. Regret is defined as the difference in the total expected reward between the best fixed action and actions induced by the policy. Regret is typically studied as a function of the time horizon $T$, which in our model is the number of agents. For multi-armed bandits, $o(T)$ regret bounds are deemed non-trivial, and $O(\sqrt{T})$ regret bounds are optimal in the worst case. Regret bounds that depend on a particular problem instance are also considered. A crucial parameter then is the \emph{gap} $\Delta$, the difference between the best and second best expected reward. One can achieve $O(\tfrac{1}{\Delta}\; \log T)$ regret rate, without knowing the $\Delta$.

\xhdr{Our results and techniques.}
Our main result is a transitive, unbiased-subhistory policy which attains near-optimal $\tilde{O}(\sqrt{T})$ regret rate for a constant number of actions. This policy also obtains the optimal instance-dependent regret rate
    $\tilde{O}(\tfrac{1}{\Delta})$
for problem instances with gap $\Delta$, without knowing the $\Delta$ in advance. In particular, we match the best possible regret rates for the multi-armed bandit problem (same as the prior work on unrestricted disclosure policies \cite{ICexploration-ec15-working}).
\asedit{Each agent $t$ sees a substantial fraction of history available at time $t$: namely, our policy reveals a subhistory of
size at least $\Omega( t/\polylog(T))$.
Our regret bounds scale exponentially in the number of actions; this dependence is grossly suboptimal for multi-armed bandits, but matches the state-of-art for unrestricted disclosure policies \cite{ICexploration-ec15-working}, and is conjectured to be unavoidable for incentivized exploration.}

The main challenge is that the agents still follow exploitation-only behavior, just like they do for the full-disclosure policy, albeit based only on a portion of history. A disclosure policy controls the flow of information (who sees what), but not the \emph{content} of that information.

The first step is to obtain any substantial improvement over the full-disclosure policy. We accomplish this with a relatively simple policy which runs the full-disclosure policy ``in parallel" on several disjoint subsets of agents,  collects all data from these runs and discloses it to all remaining agents. In practice, these subsets may correspond to multiple ``focus groups". While any single run of the full-disclosure policy may get stuck on a suboptimal arm, having these parallel runs ensure that sufficiently many of them will ``get lucky" and provide some exploration. This simple policy achieves $\tilde{O}(T^{2/3})$ regret. Conceptually, it implements a basic bandit algorithm that explores uniformly for a pre-set number of rounds, then picks one arm for exploitation and stays with it for the remaining rounds. We think of this policy  as having two ``levels": Level 1 contains the parallel runs, and Level 2 is everything else.

The next step is to implement \emph{adaptive exploration}, where the exploration schedule is adapted to previous observations. This is needed to improve over the $\tilde{O}(T^{2/3})$ regret. As a proof of concept, we focus on the case of two actions, and upgrade the simple two-level policy with a middle level. The agents in this new level receive the data collected in some (but not all) runs from the first level. What happens is that these agents explore only if the gap $\Delta$ between the best and second-best arm is sufficiently small, and exploit otherwise. When $\Delta$ is small, the runs in the first level do not have sufficient time to distinguish the two arms before herding on one of them.  However, for each of these arms, there is some chance that it has an empirical mean reward significantly above its actual mean while the other arm has empirical mean reward significantly below its actual mean in any given first-level run.  The middle-level agents observing such runs will be induced to further explore that arm, collecting enough samples for the third-level agents to distinguish the two arms. The main result extends this construction to multiple levels, connected in fairly intricate ways, obtaining optimal regreat of $\tilde{O}(T^{1/2})$.


\ascomment{The following xhdr is new}

\xhdr{Discussion.}
We argue that our subhistory-based policies require substantially weaker trust and rationality assumptions compared to the minimal-disclosure policies in prior work. There are several distinct issues here:

\begin{itemize}

\item \emph{Whether agents sufficiently understand the announced policy.} We only need an agent to understand that he is given some unbiased history. It does not matter what is the subset of rounds and how it is related to the other agents' subsets. This is arguably quite comprehensible, compared to a full-blown spec of a bandit algorithm.

\item \emph{Whether agents trust the principal's intent to implement the stated policy.} A third party can, at least in principle, collect subhistories from multiple agents and check them for consistency (e.g., that arms' average rewards are within the statistical deviations). This should create incentives for the principal not to manipulate the policy. Note that similar checks appear virtually impossible for minimal-disclosure policies.


\item \emph{Whether agents trust the principal to implement the stated policy without bugs.}
    Faithfully revealing a subhistory is arguably easy, whereas (as noted above) debugging an actual bandit algorithm in a large-scale production-level system tends to be quite complicated.

\item \emph{Whether agents react to the policy in a way that we assume.}
In our framework, an agent should react to the subhistory as if it was a set of data points collected by an algorithm, free from other agents' influence. The system can provide summary statistics, so that agents would not need to actually look at the raw data.%
\footnote{Trusting the summary statistics is a relatively minor issue, one where "cheating" can, in principle, be easily verified.}
Whereas verifying that a minimal-disclosure policy is indeed incentive-compatible typically requires a sophisticated Bayesian reasoning.

\end{itemize}

Even though we are not showing full histories, even a small fraction of history would realistically contain a large number of datapoints. Besides, a user usually looks at a small sample of reviews. In our case, one could say that these reviews are just pre-selected by the algorithm (in an ``unbiased" and ``transitive" way, as explained above).

For the sake of the analysis, we assume that the mean rewards lie in the interval
    $[\nicefrac{1}{3}, \nicefrac{2}{3}]$,
whereas the agents' response model does not take this fact into account.%
\footnote{Of course, the $[\nicefrac{1}{3}, \nicefrac{2}{3}]$ restriction can be replaced with $[\eps,1-\eps]$ for any given global constant $\eps>0$ (which would then propagate into the regret bounds).}
Our justification is two-fold. First, agents may operate under incomplete information: they simply do not know that mean rewards are restricted. Second, typical users of a recommendation system are unlikely to be fully rational (whether Bayesian or not). Following empirical averages appears much more natural. In particular, a realistic user should not estimate mean rewards as $\nicefrac{1}{3}$ if it keeps getting $0$ rewards.

On a final note, we emphasize that we provide the first result on incentivized exploration that does not rely on the strong assumptions of trust and rationality.







\input{related-work}

