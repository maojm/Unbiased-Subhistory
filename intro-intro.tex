


In the classic literature on multi-armed bandits, an agent repeatedly selects one of a set of actions, each of which has a payoff drawn from an unknown fixed distribution.  Over time, she can trade off {\em exploitation}, in which she picks an action to maximize her expected reward, with {\em exploration}, in which she takes potentially sub-optimal actions to learn more about their rewards.  By coordinating her actions across time, she can guarantee an average reward which converges to that of the optimal action in hindsight at a rate proportional to the inverse square-root of the time horizon.

In many decision problems of interest, the actions are not chosen by a single agent, as above, but rather a sequence of agents.  In such settings, each agent will choose an exploitive action as the benefits of explorative actions are only accrued by future agents.  For example, in online retail, products are purchased by a sequence of customers, each of which buys what she estimates to be the best available product.  This behavior can cause herding, in which all agents eventually take a sub-optimal action of maximum expected payoff given the available information.

This situation can be circumvented by a centralized algorithm that induces agents to take explorative actions, a line of work called {\em incentivizing exploration}.  One way to induce exploration is to introduce payments~\cite{Frazier-ec14,Kempe-colt18}. For example, an online retailer can give coupons to agents for trying certain products.  When payments are financially or technologically infeasible, another alternative is to rely on {\em information asymmetry}, \eg \cite{Kremer-JPE14,Che-13,ICexploration-ec15,Bimpikis-exploration-ms17}.
Here the idea is that the centralized algorithm can choose to selectively release information about the past actions and rewards to the agents in the form of a {\em message}.  Importantly, agents can not directly observe the past, but only learn about it through this message.  The agent then chooses an action, using the content of the message as input.  These systems are often referred to as {\em recommendation systems} as the content of the message can be interpreted as a recommendation for a particular action.

\ascomment{Nicole: It would be good to introduce "disclosure policy" (= online algorithm that chooses messages, in the context of incentivizing exploration). And also talk about recommendation system as smth that exists regardless of incentivizing exploration, and is very common/important out there.}

\OMIT{ %%% Alex's original text
Recommendation systems are ubiquitous, providing recommendations for movies (\eg  Netflix), products (\eg  Amazon), restaurants (\eg  Yelp), vacations (\eg Tripadvisor), etc.  A typical recommendation system elicits user feedback about their experiences, and aggregates this feedback in order to provide better recommendations in the future. Thus, each user she consumes information from the previous users (indirectly, \eg via recommendations), and produces new information (\eg  a review) that benefits future users. This dual role creates a three-way tension between exploration, exploitation, and users' incentives.
A social planner would balance ``exploration" of insufficiently known alternatives and ``exploitation" of the information acquired so far. Designing algorithms to trade off these two objectives is a well-researched subject in machine learning and operations research. However, a given user who decides to ``explore" typically suffers all the downside of this decision, whereas the upside (improved recommendations) is spread over many users in the future. Therefore, users' incentives are skewed in favor of exploitation. As a result, observations may be collected at a slower rate, and may suffer from selection bias (\eg  ratings of a particular movie may mostly come from people who like this type of movies). Moreover, in some natural but idealized models (\eg  \cite{Kremer-JPE14,ICexploration-ec15}), there are simple  examples when optimal recommendations are never found because the corresponding actions are never taken.

Thus, we have a problem of \emph{incentivizing exploration}. Providing monetary incentives can be financially or technologically unfeasible, and relying on voluntary exploration can lead to selection biases. A recent line of work, started by \cite{Kremer-JPE14}, relies on the inherent \emph{information asymmetry} between the recommendation system and a user. These papers posit a simple model, termed \emph{Bayesian Exploration} in \cite{ICexplorationGames-ec16}. The recommendation system is a ``principal" that interacts with self-interested ``agents" who arrive one by one. Each agent needs to make a decision: take an action from a given set of alternatives. The principal sends a \emph{message} to the agent according to some ``disclosure policy", \eg issues a recommendation. Then the agent chooses an action, and both the agent and the principal observe the outcome.  Crucially, the principal does not control the agent's decision. The problem is to design the disclosure policy for the principal that learns over time to issue messages so as to incentivize the agents to balance exploration and exploitation in a socially optimal way.
A single round of this model is a version of a well-known ``Bayesian Persuasion game" \cite{Kamenica-aer11}.
} %%%%%%%%