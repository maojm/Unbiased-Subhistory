

\nicomment{a proposed abstract since i don't have the token on main.tex:}

In a social learning setting, there is a set of actions, each of which has a payoff that depends on a hidden state of the world.  A sequence of agents each choose an action with the goal of maximizing payoff given estimates of the state of the world.  A recommendation algorithm tries to coordinate the choices of the agents by sending messages about the history of past actions.  The goal of the algorithm is to minimize the regret of the action sequence.

In this paper, we study a particular class of recommendation algorithms that use messages, called {\em unbiased subhistories}, consisting of the actions and rewards chosen by a subsequence of past agents.  One trivial message of this form contains the full history; a recommendation algorithm that chooses to use such messages risks inducing herding behavior among the agents and thus has regret linear in the number of rounds.  Our main result is a recommendation algorithm using unbiased subhistories that obtains regret $\tilde{O}(\sqrt{T})$.  We also exhibit simpler policies with higher, but still sublinear, regret.  These policies can be interpreted as dividing a sublinear number of agents into constant-sized focus groups, whose histories are then fed to future agents.

\nicomment{and now for the intro to the intro.}

In {\em incentivized exploration}, a recommendation algorithm presents a sequence of agents 

In social learning, a sequence of individuals choose actions based on observations of past choices.  Incentivized exploration algorithms coordinate social learning, hiding inducing sufficient exploration





\ascomment{Nicole, it is all yours ..}

Recommendation systems are ubiquitous, providing recommendations for movies (\eg  Netflix), products (\eg  Amazon), restaurants (\eg  Yelp), vacations (\eg Tripadvisor), etc.  A typical recommendation system elicits user feedback about their experiences, and aggregates this feedback in order to provide better recommendations in the future. Thus, each user she consumes information from the previous users (indirectly, \eg via recommendations), and produces new information (\eg  a review) that benefits future users. This dual role creates a three-way tension between exploration, exploitation, and users' incentives.
A social planner would balance ``exploration" of insufficiently known alternatives and ``exploitation" of the information acquired so far. Designing algorithms to trade off these two objectives is a well-researched subject in machine learning and operations research. However, a given user who decides to ``explore" typically suffers all the downside of this decision, whereas the upside (improved recommendations) is spread over many users in the future. Therefore, users' incentives are skewed in favor of exploitation. As a result, observations may be collected at a slower rate, and may suffer from selection bias (\eg  ratings of a particular movie may mostly come from people who like this type of movies). Moreover, in some natural but idealized models (\eg  \cite{Kremer-JPE14,ICexploration-ec15}), there are simple  examples when optimal recommendations are never found because the corresponding actions are never taken.

Thus, we have a problem of \emph{incentivizing exploration}. Providing monetary incentives can be financially or technologically unfeasible, and relying on voluntary exploration can lead to selection biases. A recent line of work, started by \cite{Kremer-JPE14}, relies on the inherent \emph{information asymmetry} between the recommendation system and a user. These papers posit a simple model, termed \emph{Bayesian Exploration} in \cite{ICexplorationGames-ec16}. The recommendation system is a ``principal" that interacts with self-interested ``agents" who arrive one by one. Each agent needs to make a decision: take an action from a given set of alternatives. The principal sends a \emph{message} to the agent according to some ``disclosure policy", \eg issues a recommendation. Then the agent chooses an action, and both the agent and the principal observe the outcome.  Crucially, the principal does not control the agent's decision. The problem is to design the disclosure policy for the principal that learns over time to issue messages so as to incentivize the agents to balance exploration and exploitation in a socially optimal way.
A single round of this model is a version of a well-known ``Bayesian Persuasion game" \cite{Kamenica-aer11}.

\ascomment{ ... up to here.}