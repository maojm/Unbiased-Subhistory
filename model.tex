%!TEX root = main.tex
\section{Model}
\label{sec:model}

We study the mult-armed bandit problem in a social learning context, in which a principal faces a sequence of $T$ myopic agents. There is a set $\A$ of $K$ arms, and the reward from pulling an arm $a\in \A$ is distributed according to the Bernoulli distribution $\cD_a$ with an unknown mean $\mu_a$. At each round $t\in [T]$, a new agent $t$ arrives, receives a message $m_t$ from the principal, chooses an arm $a_t\in \A$, and collects a reward $r_t\in \{0,1\}$ that is immediately observed by the principal. Given any sequence of arms $\{a_1, \ldots, a_T\}$ chosen by the agents, the \emph{regret} is defined as
\[
  \reg(T)= \reg(a_1, \ldots , a_T) = T \max_{a\in \A} \mu_a -
  \sum_{t=1}^T \mu_{a_t}.
\]

\xhdr{Unbiased sub-history policy.} The principal's goal is to reveal a sequence of messages according to a disclosure policy so that the regret of the induced sequence of actions is minimized. We will focus on \emph{unbiased sub-history policies} that at each round $t$ reveal the \emph{sub-history} $H_t = \{(s, a_s, r_s)\mid s \in S_t\}$, such that each $S_t\subseteq [t-1]$ is a subset of previous rounds \emph{fixed ahead of time}. A simple example is the policy that reveals the \emph{full history} with $S_t = [t-1]$.  All of the policies we develop are \emph{transitive}: for any two agents $t$ and $t'$ such that $t'\in S_t$, then the sub-history of $S_t$ contains that of $S_{t'}$.

\xhdr{Information flow graph.} We will describe our transitive policies using a \emph{information flow graph} (and henceforth \emph{information graph})---a directed graph over the $T$ agents such that if there is an edge from $u$ to $v$, it means $u\in S_v$. By the property of transitivity, $S_t$ is the set of nodes that can reach node $t$ in the info graph. \niedit{For example, an information graph for the policy that reveals the full history is the directed line graph containing edges $(t,t+1)$ for all $t\in[T-1]$.}

\iffalse
\begin{itemize}
\item \textbf{Full history:} We show each agent the history of all previous agents' pulls. We call this policy \ALGG. 
\item \textbf{Unbiased sub-history:} We show each agent only a subset of history. The subsets are pre-determined before any arms are pulled.  
\item \textbf{Info graph:} All the recommendation policies in this paper can be described by an undirected transitive graph with $T$ nodes. We call it the info graph. The $T$ nodes in the graph represent the $T$ agents. If there is an edge between node $u$ and $v$ for $u < v$, it means the history of agent $u$ is shown to agent $v$.
\end{itemize}
\fi

\begin{remark}
  Why not arbitrary sub-history? \swcomment{important point to make;
    but should be in the introduction}
\end{remark}

\xhdr{Agents' behavior.} \nicomment{re-ordered this.} An {\em agent estimator} is a function that maps the revealed sub-history $H_t$ to a mean reward estimate $\hat{\mu}_a^t$ for every arm $a$. For any $t\in[T]$ and $a\in \A$, we write $N_{t, a}$ and $\bar{\mu}_a^t$ to denote the number of pulls of $a$ and the empirical mean reward of arm $a$ in sub-history $H_t$ respectively. If an arm is not pulled in $H_t$, we will let $\bar{\mu}_a^t = 0$.  We require the following assumptions.

\begin{assumption}
	\label{ass:embehave}
	The set of mean rewards $\{\mu_a\}$, the estimates $\hat \mu_a^t$, and the empirical mean rewards $\bar\mu_a^t$  satisfy:
	
	\begin{itemize}
		\item The mean reward $\mu_a$ for each arm $a\in \A$ lies in the range
		of $[1/3, 2/3]$.
		
		
		\item Let $c_m = 1/16$.\swcomment{maybe use a better notation} \nicomment{yeah, why the subscript $m$? and $T$ below? how about just $\alpha,\beta$ for the constants in this assumption? or $\alpha$ and $N_0$? or use $n_{t,a}$ for number of pulls and $N$ for a constant that you want the $n_{t,a}$ to exceed?}
		% \jmcomment{It means a constant for some assumptions about means but nothing else. Just want to make it different from other constants. Maybe there are better notations.}. 
		There exists a constant $c_T$ such that for any agent $t$ and
		arm $a$, as long as \niedit{$N_{t,a} \geq c_T$}, 
		\[
		\left|\hat{\mu}^t_a - \bar{\mu}^t_a \right| <
		\frac{c_m}{\sqrt{N_{t,a}}}.
		\]
		\item If $N_{t,a} = 0$, the initial estimate
		$\hat{\mu}^t_a$ is at least $1/3$.
	\end{itemize}
\end{assumption}

The first assumption ensures that the reward distributions have sufficient entropy to induce natural exploration. We choose the bounded range $[1/3, 2/3]$ for the simplicity of our analysis, and it can further relaxed to $[1/C, (C-1)/C]$ for any constant $C$. The second assumption says that the estimates computed by the agents are well-behaved, and are close to the empirical estimates given by the sub-history, provided that the number of observations is sufficiently large.

We model agents with heterogeneity in their arm selections. In particular, there is an unknown distribution over the set of agent estimators satisfying Assumption~\ref{ass:embehave}.  Each agent $t$ indepedently draws an estimator from this distribution, uses it to calculate the mean reward estimates $\hat{\mu}_a^t$ for every arm $a$, and then chooses the arm $a_t$ with the highest estimate. 
% \swcomment{why independently? can be be
%   adversarial?} \jmcomment{Actually current proofs don't work when it's adversarial. It's fairly annoying for the anti-concentration argument. In the proof, we need to use independent to show that the amount of agents pulling arm $a$ in the first level is concentrated to $T_1 q_a$.}

\begin{remark}
We emphasize that the agents we consider in this paper are {\em frequentists}.  Thus their estimators, which determine their behavior, take samples as inputs and not priors.  The estimators satisfying Assumption~\ref{ass:embehave} include that of the natural greedy frequentist, who always pulls the arm with the highest empirical mean.
\end{remark}

Given any bandit instance, we define the \emph{gap parameter} $\Delta$ as the difference between the largest and second largest mean rewards. In general, the gap parameter is unknown to the principal (or the algorithm), but we show how to obtain different forms of regret bounds if $\Delta$ is known.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
