%!TEX root = main.tex
\section{Our Model}
\label{sec:model}

\emph{Bayesian Exploration} is a game between a principal and $T$ agents who arrive one by one. Each round $t$ proceeds as follows: a new agent $t$ arrives, receives a message $m_t$ from the principal, chooses an action $a_t$ from a fixed action space $\A = \{1,...,K\}$, and collects a reward $r_t\in \{0,1\}$ that is immediately observed by the principal. 

Each action $a \in \A$ has a state $\mu_a$ drawn from $\D_a$ before the game starts. For agent in round $t$ who takes action $a_t$, the reward $r_t$ is distributed as $\Ber(\mu_{a_t})$.  

The messages $m_t$ are generated according to a randomized online algorithm $\pi$ termed ``recommendation policy".

\xhdr{Restrictions on the messages.} We classify recommendation policies according to what messages are allowed. 
\begin{itemize}
\item Full History: 
\item Unbiased Sub-history: 
\end{itemize}

\begin{remark}
Why not arbitrary sub-history?
\end{remark}

\xhdr{Agents behavior.} We assume agent $t$ estimates the mean of each arm as $\hat{\mu}_a^t$'s and pick the arm with the highest $\hat{\mu}_a^t$'s. We also define $\bar{\mu}_a^t$ as the empirical average of arm $a$ given the history. We assume each agent's estimate $\hat{\mu}_a^t$ is close to the empirical average $\bar{\mu}_a^t$, more specifically as the following assumption:
\begin{assumption}
\label{ass:embehave}
Let constant $c_m = 1/16$. There exists a constant $c_T$ such that the following is true. For each agent $t$ and arm $a$, let $t_a$ be the number of times arm $a$ has been pulled in the history observed by agent $t$. 
\begin{itemize}
\item \textbf{Close to the empirical average:} If $t_a \geq c_T$, 
\[
|\hat{\mu}^t_a - \bar{\mu}^t_a | < \frac{c_m}{\sqrt{t_a}}.
\]
\item \textbf{Initial value:} If $t_a = 0$, $\hat{\mu}^t_a$ is at least $1/3$. 
\end{itemize}
\end{assumption}

We allow different agents to have different behavior models as long as they satisfy Assumption \ref{ass:embehave}. More specifically, we assume each agent's behavior model is sampled from an unknown distribution independently. 
