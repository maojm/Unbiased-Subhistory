%!TEX root = main.tex
\section{Our Model}
\label{sec:model}

\emph{Incentivizing Exploration} is a game between a principal and $T$ agents who arrive one by one. Each round $t$ proceeds as follows: a new agent $t$ arrives, receives a message $m_t$ from the principal, chooses an arm $a_t$ from a fixed arm space $\A = \{1,...,K\}$, and collects a reward $r_t\in \{0,1\}$ that is immediately observed by the principal. 
We mostly focus on the case when $K=2$. We also discuss how to generalize our results to the case when $K$ is a constant greater than 2.

Each arm $a \in \A$ has a state $\mu_a$ drawn from $\D_a$ before the game starts. For the agent in round $t$ who picks arm $a_t$, the reward $r_t$ is distributed as $\Ber(\mu_{a_t})$.\swcomment{do we plan to generalize to real-valued?}

The messages $m_t$ are generated according to a randomized online algorithm $\pi$ termed ``recommendation policy".

\xhdr{Restrictions on the messages.} We classify recommendation policies according to what messages are allowed. 
\begin{itemize}
\item \textbf{Full history:} We show each agent the history of all previous agents' pulls. We call this policy \ALGG. 
\item \textbf{Unbiased sub-history:} We show each agent only a subset of history. The subsets are pre-determined before any arms are pulled.  
\item \textbf{Info graph:} All the recommendation policies in this paper can be described by an undirected transitive graph with $T$ nodes. We call it the info graph. The $T$ nodes in the graph represent the $T$ agents. If there is an edge between node $u$ and $v$ for $u < v$, it means the history of agent $u$ is shown to agent $v$.
\end{itemize}

\begin{remark}
Why not arbitrary sub-history?
\end{remark}

\xhdr{Agents' behavior.} We assume agent $t$ estimates the mean of each arm as $\hat{\mu}_a^t$'s and picks the arm with the highest $\hat{\mu}_a^t$. We define $\bar{\mu}_a^t$ as the empirical average of arm $a$ given the history\swcomment{sub-history he receives?}. We assume each agent's estimate $\hat{\mu}_a^t$ is close to the empirical average $\bar{\mu}_a^t$, more specifically as the following assumption:
\begin{assumption}
\label{ass:embehave}
Set constant $c_m = 1/16$. There exists a constant $c_T$ such that the following is true. For each agent $t$ and arm $a$, let $t_a$ be the number of times arm $a$ has been pulled in the history observed by agent $t$. 
\begin{itemize}
\item \textbf{Close to the empirical average:} If $t_a \geq c_T$, 
\[
|\hat{\mu}^t_a - \bar{\mu}^t_a | < \frac{c_m}{\sqrt{t_a}}.
\]
\item \textbf{Initial value:} If $t_a = 0$, $\hat{\mu}^t_a$ is at least $1/3$. 
\end{itemize}
\end{assumption}

We allow different agents to have different behavior models as long as they satisfy Assumption \ref{ass:embehave}. More specifically, we assume each agent's behavior model is sampled from an unknown distribution independently. 
\begin{remark}
how this relates to Bayesian exploration with conjugate priors
\end{remark}