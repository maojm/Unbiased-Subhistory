%!TEX root = main.tex
\section{Our Model}
\label{sec:model}

We study the mult-armed bandit problem in a social learning context,
in which a principal faces a sequence of $T$ myopic agents. There is a
set $\A$ of $K$ arms, and the reward from pulling an arm $a\in \A$ is
distributed according to the Bernoulli distribution with an unknown
mean $\mu_a$.\swcomment{do we plan to generalize to real-valued?}  At
each round $t\in [T]$, a new agent $t$ arrives, receives a message
$m_t$ from the principal, chooses an arm $a_t\in \A$, and collects a
reward $r_t\in \{0,1\}$ that is immediately observed by the
principal. Given any sequence of arms $\{a_1, \ldots, a_T\}$ chosen by
the agents, the \emph{regret} is defined as
\[
  \reg(T)= \reg(a_1, \ldots , a_T) = T \max_{a\in \A} \mu_a -
  \sum_{t=1}^T \mu_{a_t}.
\]




% We mostly focus on the case when $K=2$. We also discuss how to
% generalize our results to the case when $K$ is a constant greater
% than 2. The messages $m_t$ are generated according to a randomized
% online algorithm $\pi$ termed ``recommendation policy".

\xhdr{Unbiased sub-history policy.} The principal's goal is to reveals
a sequence of messages accroding to a disclosure policy so that the
regret of the induced sequence of actions is minimized. We will focus
on \emph{unbiased sub-history policies} that at each round $t$ reveal
the \emph{sub-history} in a subset of previous rounds
$S_t\subseteq [t-1]$: $\{(s, a_s, r_s)\mid s \in S_t\}$, where the
subset $S_t$ is fixed ahead of time. A simple example is the policy
that reveals the \emph{full history}, that is $S_t = [t-1]$.  All of
the policies we develop are \emph{transitive}: for any two agents $t$
and $t'$ such that $t'\in S_t$, then the sub-history of $S_t$ contains
that of $S_{t'}$.


\xhdr{Information flow graph.} We will describe our transitive
policies using a \emph{information flow graph} (and henceforth
\emph{info graph})---a directed graph over the $T$ agents such that if
there is an edge from $u$ to $v$, it means $u\in S_v$. By the property
of transitivity, $S_t$ is the set of nodes that can reach node $t$ in
the info graph.

\iffalse
\begin{itemize}
\item \textbf{Full history:} We show each agent the history of all previous agents' pulls. We call this policy \ALGG. 
\item \textbf{Unbiased sub-history:} We show each agent only a subset of history. The subsets are pre-determined before any arms are pulled.  
\item \textbf{Info graph:} All the recommendation policies in this paper can be described by an undirected transitive graph with $T$ nodes. We call it the info graph. The $T$ nodes in the graph represent the $T$ agents. If there is an edge between node $u$ and $v$ for $u < v$, it means the history of agent $u$ is shown to agent $v$.
\end{itemize}
\fi

\begin{remark}
  Why not arbitrary sub-history? \swcomment{important point to make;
    but should be in the introduction}
\end{remark}

\xhdr{Agents' behavior.} Based on the history over rounds $S_t$, each
agent $t$ first computes an estimate $\hat{\mu}_a^t$ for the mean
reward of each arm $a$, and then selects the arm with the highest
$\hat{\mu}_a^t$. Let $t_a$ be the number of times arm $a$ is pulled in
the observed history $S_t$. For any $a$ with $t_a > 0$, we write
$\bar{\mu}_a^t$ as the empirical average of arm $a$ given the history.

% We assume that for sufficiently high $t_a$, the estimate
% $\hat{\mu}_a^t$ is close to the empirical average $\bar{\mu}_a^t$.

We will make the following assumptions on the set of mean rewards
$\{\mu_a\}$, and the relationship between the estimates $\hat \mu_a^t$
.and empirical mean rewards $\bar\mu_a^t$.



\begin{assumption}
\label{ass:embehave}
Let $c_m = 1/16$.\swcomment{subscript $m$ meant anything?}. 
\begin{itemize}
\item The mean reward $\mu_a$ for each arm $a\in \A$ lies in the range
  of $[1/3, 2/3]$.


\item There exists a constant $c_T$ such that for any agent $t$ and
  arm $a$, as long as $t_a \geq c_T$,
\[
  \left|\hat{\mu}^t_a - \bar{\mu}^t_a \right| <
  \frac{c_m}{\sqrt{t_a}}.
\]
\item In the case of $t_a = 0$, the initial estimate $\hat{\mu}^t_a$
  is at least $1/3$.
\end{itemize}
\end{assumption}


The first assumption ensures that the reward distributions have
sufficient entropy to induce natural exploration. We choose the
bounded range $[1/3, 2/3]$ for the simplicity of our analysis, and it
can further relaxed to $[1/C, (C-1)/C]$ for any constant $C$. The
second assumption says that the estimates computed by the agents are
well-behaved, and are close to the empirical estimates given by the
sub-history, provided that the number of observations is sufficiently
large.


We allow different agents to have different behavior models as long as
they satisfy Assumption \ref{ass:embehave}. More specifically, we
assume each agent's behavior model is sampled from an unknown
distribution independently.\swcomment{why independently? can be be
  adversarial?}
\begin{remark}
  how this relates to Bayesian exploration with conjugate
  priors\swcomment{seems to be something nasty}
\end{remark}

Given any bandit instance, we can also define the \emph{gap parameter}
$\Delta$ as the difference between the largest and second largest mean
rewards. In general, the gap parameter is unknown to the principal (or
the algorithm), but we also show how we can obtain different forms of
regret bounds if $\Delta$ is known.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
