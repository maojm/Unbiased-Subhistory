%!TEX root = main.tex
\section{Model and Preliminaries}
\label{sec:model}

\newcommand{\SubH}[1]{H_{#1}} % subhistory
\newcommand{\AnonSubH}[1]{H^{\texttt{anon}}_{#1}} % subhistory


We study the multi-armed bandit problem in a social learning context, in which a principal faces a sequence of $T$ myopic agents. There is a set $\A$ of $K$ possible actions, a.k.a. \emph{arms}. At each round $t\in [T]$, a new agent $t$ arrives, receives a message $m_t$ from the principal, chooses an arm $a_t\in \A$, and collects a reward $r_t\in \{0,1\}$ that is immediately observed by the principal. The reward from pulling an arm $a\in \A$ is drawn independently from Bernoulli distribution $\cD_a$ with an unknown mean $\mu_a$. An agent does not observe anything from the previous rounds, other than the message $m_t$. The problem instance is defined by (known) parameters $K,T$ and the (unknown) tuple of mean rewards, $(\mu_a:\,a\in\A)$. We are interested in \emph{regret}, defined as
\[\textstyle 
  \reg(T)
  %= \reg(a_1, \ldots , a_T) 
  = T \max_{a\in \A} \mu_a -
  \sum_{t\in [T]} \E[\mu_{a_t}].
\]
The principal chooses messages $m_t$ according to an online algorithm called \emph{disclosure policy}, with a goal to minimize regret. We assume that mean rewards are bounded away from $0$ and $1$, to ensure sufficient entropy in rewards. For concreteness, we posit that
    $\mu_a\in [\tfrac13,\tfrac23]$.

\xhdr{Unbiased subhistories.}  
The \emph{subhistory} for a subset of rounds $S\subset [T]$, and the corresponding \emph{anonymized subhistory} are defined, respectively, as
\[ \SubH{S} = \{\; (s,a_s,r_s):\;s\in S \;\}
\quad\text{and}\quad
   \AnonSubH{S} = \{\; (a_s,r_s):\;s\in S \}.
\]
Accordingly, $\SubH{[t-1]}$ is called the \emph{full history} at time $t$.


We focus on disclosure policies of a particular form, where the message in each round $t$ is $m_t = \SubH{S_t}$ for some subset $S_t\subset [t-1]$. We assume that the subset $S_t$ is chosen ahead of time, before round $1$ (and therefore does not depend on the observations $\SubH{t-1}$). Such message is called \emph{unbiased subhistory}, and the resulting disclosure policy is called an \emph{unbiased-history policy}.

Further, we assume that disclosure policies are \emph{transitive}, in the following sense:
\[ t'\in S_t \Rightarrow S_{t'}\subset S_t
    \qquad \text{for all rounds $t,t'\in [T]$}. \]
In words, if agent observes the subhistory for some previous agent, then she observes the entire message for that agent. 

A transitive unbiased-history policy $\pi$ can be represented as an undirected graph $G_\pi$, where nodes correspond to rounds, and any two rounds $t'<t$ are connected if and only if $t'\in S_t$ and there is no intermediate round $t''$ with
    $t'\in S_{t''}$ and $t''\in S_t$.
This graph is henceforth called the \emph{information flow graph} of policy $\pi$, or \emph{info-graph} for short.

\xhdr{Agents' behavior.} Let us define agents' behavior in response to an unbiased-history policy. We posit that each agent $t$ uses its observed subhistory $m_t$ to form a reward estimate $\hat{\mu}_{t,a} \in [0,1]$ for each arm $a\in \A$, and chooses an arm with a maximal estimator. (Ties are broken according to an arbitrary rule that is the same for all agents.)
We make some mild assumptions:

%If an arm is not pulled in $H_t$, we will let $\bar{\mu}_a^t = 0$.

\begin{assumption}\label{ass:embehave}
Reward estimates satisfy the following assumptions:
\begin{itemize}
\item[(a)] Reward estimates are close to empirical averages. Let $N_{t,a}$ and $\bar{\mu}_{t,a}$ denote the number of pulls and the empirical mean reward of arm $a$ in subhistory $m_t$. Then for some absolute constant $N_0\in \N$ and $C_0=\tfrac{1}{16}$ it holds that
\[
\forall t\in[T],\, a\in\A \qquad
N_{t,a} \geq N_0 \quad\Rightarrow\quad
    \left|\hat{\mu}^t_a - \bar{\mu}^t_a \right| <
		\frac{C_0}{\sqrt{N_{t,a}}},
\]
\item[(b)] Reward estimates have a warm-start property:
\[
\forall t\in[T],\, a\in\A \qquad
N_{t,a} =0 \quad\Rightarrow\quad
    \hat{\mu}^t_a \geq \tfrac13.
\]

\item[(c)] Reward estimates $\hat{\mu}_a^t$ depend only on the anonymized subhistory $\AnonSubH{S_t}$. That is, reward estimates depend only on the observations, not on the rounds in which these observations are collected.

\item[(c)] Reward estimates are consistent over time, but allow some agent heterogeneity. Formally, each agent $t$ forms its estimates according to a \emph{reward estimate function}: a function $f_t$ from anonymous subhistories to $[0,1]^K$, so that the estimate vector
        $(\hat{\mu}_{t,a}:\, a\in\A)$
    equals $f_t(\AnonSubH{S_t})$. And this function is drawn independently from some fixed distribution over reward estimate functions.
\end{itemize}
\end{assumption}

\OMIT{ %%%%%%%%%%
The first assumption ensures that the reward distributions have sufficient entropy to induce natural exploration. We choose the bounded range $[1/3, 2/3]$ for the simplicity of our analysis, and it can further relaxed to $[1/C, (C-1)/C]$ for any constant $C$. The second assumption says that the estimates computed by the agents are well-behaved, and are close to the empirical estimates given by the sub-history, provided that the number of observations is sufficiently large.

We model agents with heterogeneity in their arm selections. In particular, there is an unknown distribution over the set of agent estimators satisfying Assumption~\ref{ass:embehave}.  Each agent $t$ indepedently draws an estimator from this distribution, uses it to calculate the mean reward estimates $\hat{\mu}_a^t$ for every arm $a$, and then chooses the arm $a_t$ with the highest estimate.
% \swcomment{why independently? can be be
%   adversarial?} \jmcomment{Actually current proofs don't work when it's adversarial. It's fairly annoying for the anti-concentration argument. In the proof, we need to use independent to show that the amount of agents pulling arm $a$ in the first level is concentrated to $T_1 q_a$.}

\begin{remark}
We emphasize that the agents we consider in this paper are {\em frequentists}.  Thus their estimators, which determine their behavior, take samples as inputs and not priors.  The estimators satisfying Assumption~\ref{ass:embehave} include that of the natural greedy frequentist, who always pulls the arm with the highest empirical mean.
\end{remark}
} %%%%%%%

\xhdr{Connection to multi-armed bandits.}
The special case when each message $m_t$ is an arm, and the $t$-th agent always chooses this arm, corresponds to a standard multi-armed bandit problem with IID rewards. Following the literature on bandits, we define the \emph{gap parameter} $\Delta$ as the difference between the largest and second largest mean rewards.%
\footnote{Formally, the second-largest mean reward is 
    $\max_{a\in\A:\mu(a)<\mu^*} \mu(a)$,
where $\mu^* = \max_{a\in\A} \mu(a)$.
}
The gap parameter is not known to the principal (in our problem), or to the algorithm (in the bandit problem). However, it is essential for regret bounds. 

\ascomment{We'll need to state/discuss optimal regret bounds somewhere, probably here.}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
